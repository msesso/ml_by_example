{
 "cells": [
  {
   "source": [
    "conda activate mlexample_env\n",
    "\n",
    "- Both below must be in the same path\n",
    "\n",
    "which jupyter\n",
    "\n",
    "which jupyter-notebook\n",
    "\n",
    "- Then if they are not use:\n",
    "\n",
    "conda install notebook\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning by Example (2020)\n",
    "\n",
    "\"*In traditional programming, the computer follows a set of predefined rules to\n",
    "process the input data and produce the outcome. In machine learning, the computer\n",
    "tries to mimic human thinking.*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks can be classified into:\n",
    "\n",
    "1) Unsupervised Learning: Data used for learning has indicative signals but no description. Ex: Anomalies detection;\n",
    "\n",
    "2) Supervised Learning: Goal is to find a function mapping inputs to output, so in this sense data comes with description, targets or desired output. Ex: Sales forecasting;\n",
    "\n",
    "3) Reinforcement Learning: System can adapt to certain dynamic conditions with data providing feedbacks. There is a goal in the end and the system understands its perfomance, adjusting accordingly. Ex: Self-driven cars;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting, underfitting, and the bias-variance trade-off\n",
    "\n",
    "Concepts recap:\n",
    "\n",
    "**Bias**: \n",
    "\n",
    "-> Error from incorrect assumptions in learning algorithm:\n",
    "\n",
    "\\begin{align}\n",
    "Bias[ \\hat y ] = E[\\hat y - y ]\n",
    "\\end{align}\n",
    "\n",
    "**Variance**: \n",
    "\n",
    "-> Sensitivity of the model regarding variations in dataset:\n",
    "\n",
    "\\begin{align}\n",
    "Variance = E[ \\hat y^2 ] - E[\\hat y]^2\n",
    "\\end{align}\n",
    "\n",
    "**Mean Squared Error (MSE)**:\n",
    "\n",
    "-> A measure for the error of estimation\n",
    "\n",
    "\\begin{align}\n",
    "MSE = E[(y(x) - \\hat y (x))^2]\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "**Overfitting**: The model is fitting the training set extremely well, but it is not good for predictions, in this sense, it does not have \"external validity\".\n",
    "    \n",
    "- Its bias is low, but variance is high, since predictions tend to have large variability;\n",
    "    \n",
    "**Underfitting**: The model perfoms badly in training and test sets.\n",
    "\n",
    "- Its bias is high, variance potentially low (in case our model is extremely simple, think about a straight horizontal line as prediction);\n",
    "    \n",
    "**Bias-variance trade-off**\n",
    "\n",
    "More data and complex models tend to reduce bias, however there will be more shifts in the model to better fit the data, increasing variance.\n",
    "\n",
    "\\begin{align}\n",
    "MSE & = E[(y - \\hat y)^2]\\\\\n",
    "& = E \\left[(y-E[\\hat y] + E[\\hat y] - \\hat y)^2\\right]\\\\\n",
    "& = E \\left[(y-E[\\hat y])^2 \\right] + E\\left[(E[\\hat y] - \\hat y)^2\\right] + E\\left[2\\left(y-E[\\hat y]\\right)\\left(E[\\hat y] - \\hat y\\right)\\right]\\\\\n",
    "& = E \\left[(y-E[\\hat y])^2 \\right] + E\\left[(E[\\hat y] - \\hat y)^2\\right] + 2\\left(y-E[\\hat y]\\right)\\left(E[\\hat y] - E[\\hat y]\\right)\\\\\n",
    "& = \\left(E[\\hat y - y]\\right)^2 + E[\\hat y^2] - E[\\hat y ]^2\\\\\n",
    "& = \\underbrace{Bias[ \\hat y ]^2}_{\\text{Error of estimations}} + \\underbrace{Variance[ \\hat y ]}_{\\hat y \\text{ movement around its mean}}\n",
    "\\end{align}\n",
    "    \n",
    "**Cross-validation**\n",
    "\n",
    "Cross-validation helps in avoiding overfitting, such that the training set is split into training and validation set. \n",
    "\n",
    "It can be: (1) **exhaustive**: When all possible partitions are tested (e.g. Leave-One-Out-Cross_Validation - LOOCV); (2) **Non-exhaustive**: Not all possible partitions are used (e.g. k-fold cross validation - set is split in k equal-size folds leaving one out for test in each of the k rounds);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: Building a Movie Recommendation Engine with Naive Bayes\n",
    "\n",
    "- Movie recommendation is a classification problem.\n",
    "\n",
    "- Generally speaking classification maps observations/features/predictive variables to target categories/labels/classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classification\n",
    "\n",
    "- Classify observations in one of two possible classes (e.g. spam detection, click-thorugh for online ads, whether a person likes or not a movie).\n",
    "\n",
    "### Multiclass Classification\n",
    "\n",
    "- Classify observations in more than two possible classes (e.g. handwritten digit recognition as number 9, 2, etc).\n",
    "\n",
    "### Multi-label Classification\n",
    "\n",
    "- An observation can belong to more than one class (e.g. a movie can be classified as adventure, sci-fi).\n",
    "\n",
    "- Typical approach to solve is divide it in a set of binary problem classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Naive Bayes\n",
    "\n",
    "- Probabilistic classifier\n",
    "\n",
    "#### Recall Bayes' Theorem\n",
    "\n",
    "$$P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$$\n",
    "\n",
    "E.g. If I have a unfair coin (U) and a fair one (F), such that in the first one the probability of head is 90% (P(H|U)=90%). Given that we got head, what it the probability that an unfair coin was picked?\n",
    "\n",
    "Answer: $$P(U|H) = \\frac{P(H|U)P(U)}{P(H)} = \\frac{P(H|U)P(U)}{P(H|U)P(U) + P(H|F)P(F)} = \\frac{0.9*0.5}{0.9*0.5+0.5*0.5} = 0.64$$\n",
    "\n",
    "#### The mecanics of Naive Bayes\n",
    "\n",
    "Consider:\n",
    "\n",
    "Let $k \\in \\{1,2,...,K\\}$ denote classes, the probability that a sample belong to class $k$ given observed $x$ is: \n",
    "\n",
    "$$P(y_k|x) = \\frac{P(x|y_k)P(y_k)}{P(x)}$$\n",
    "\n",
    "The names given for the components of the equation above are:\n",
    "\n",
    "- **Prior**: $P(y_k)$ - How classes are distributed, without any knowledge of features;\n",
    "\n",
    "- **Posterior**: $P(y_k|x)$ - Incorporates knowledge from observation;\n",
    "\n",
    "- **Likelihood** - $P(x|y_k)$ - The distribution of n features given that the sample belong to class $y_k$. \n",
    "Likelihood ends up being very hard to calculate when there are a large number of features, since this become a large joint distribution.\n",
    "To circumvent this issue, Naive Bayes assumes feature independence, which allow us to write:\n",
    "\n",
    "$$P(x|y_k) = P(x_1|y_k)*P(x_2|y_k)*...*P(x_n|y_k)$$\n",
    "\n",
    "The denominator of our bayes formula, $P(x)$ (called **evidence**) depends on overall distribution of features, meaning that it acts a constant, \n",
    "and so our posterior is proportional to:\n",
    "\n",
    "$$ P(y_k|x) \\propto P(x|y_k)P(y_k) = P(x_1|y_k)*P(x_2|y_k)*...*P(x_n|y_k) $$\n",
    "\n",
    "Note that it is possible that, for a given sample, a given feature, say $n'$ presents: $P(x_{n'}|y_k) = 0$, which would cause the likelihood to be zero, \n",
    "and so an unknown likelihood. To avoid that, **Laplace Smoothing** is used:\n",
    "\n",
    "$$ P(x_{n'}|y_k) = \\frac{N_{x_{n'}|y_k} + \\alpha}{N_{y_k}+ \\alpha d}$$\n",
    "\n",
    "Where $N_{x_{n'}|y_k}$ is how many times $x_{n'}$ occured given that $y_k$ was observed, $N_{y_k}$ how many times $y_k$ was observed, $\\alpha>0$ is the smoothing parameter ($\\alpha=0$ means no \n",
    "smoothing, many times this is set to 1) and $d$ in the binary case is 2 (because there are two possible values).\n",
    "\n",
    "Knowing the likelihoods and given some prior, one can calculate the posteriors. Using the fact that the sum of posteriors for a given $x$ is 1, the probaility that\n",
    "$y_k$ is observed given $x$ is found. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Building a toy dataset, which tries to discover if the user likes the target movie based on how she likes other three movies (like is YES or NO)\n",
    "X_train = np.array([\n",
    "    [0, 1, 1],\n",
    "    [0, 0, 1],\n",
    "    [0, 0, 0],\n",
    "    [1, 1, 0]])\n",
    "Y_train = ['Y', 'N', 'Y', 'Y']\n",
    "X_test = np.array([[1, 1, 0]])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by label ('Y' and 'N') recording their indices (where they show up) by classes\n",
    "\n",
    "def get_label_indices(labels):\n",
    "    \"\"\"\n",
    "    Group samples based on their labels and return indices\n",
    "    @param labels: list of labels\n",
    "    @return: dict, {class1: [indices], class2: [indices]}\n",
    "    \"\"\"\n",
    "    from collections import defaultdict\n",
    "    label_indices = defaultdict(list)\n",
    "    for index, label in enumerate(labels):\n",
    "        #print('index:',index)\n",
    "        #print('label', label)\n",
    "        label_indices[label].append(index)\n",
    "    return label_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_indices:\n",
      " defaultdict(<class 'list'>, {'Y': [0, 2, 3], 'N': [1]})\n"
     ]
    }
   ],
   "source": [
    "label_indices = get_label_indices(Y_train)\n",
    "print('label_indices:\\n', label_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prior(label_indices):\n",
    "    \"\"\"\n",
    "    Compute prior based on training examples\n",
    "    @param label_indices: grouped sample indices by class\n",
    "    @return: dictionary, with class label as key, corresponding prior\n",
    "             as the value\n",
    "    \"\"\"\n",
    "    # define prior as an object that can be referred by label\n",
    "    #  get the length of indices inside label_indices items\n",
    "    prior = {label: len(indices) for label, indices in label_indices.items()}\n",
    "    total_count = sum(prior.values())\n",
    "    for label in prior:\n",
    "        prior[label] /= total_count\n",
    "    return prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: [0, 2, 3]\n",
      "label Y\n",
      "index: [1]\n",
      "label N\n",
      "Prior: {'Y': 0.75, 'N': 0.25}\n"
     ]
    }
   ],
   "source": [
    "for label, indices in label_indices.items(): \n",
    "    print('index:',indices)\n",
    "    print('label', label)\n",
    "\n",
    "prior = get_prior(label_indices)\n",
    "print('Prior:', prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_likelihood(features,label_indices,smoothing = 0):\n",
    "    \"\"\"\n",
    "    Compute likelihood based on training samples, using \n",
    "            Laplace approximation\n",
    "    @param features: Matrix of features\n",
    "    @param label_indices: Grouped sample indices by class\n",
    "    @param smoothing: integer, additive smoothing parameter\n",
    "    @return: dictionary, with class as keym corresponding \n",
    "             conditional probability P(feature|class) vector \n",
    "             as value\n",
    "    \"\"\"\n",
    "    likelihood = {}\n",
    "    for label, indices in label_indices.items():\n",
    "\n",
    "        likelihood[label] = features[indices, :].sum(axis=0) + smoothing\n",
    "\n",
    "        total_count = len(indices)\n",
    "\n",
    "        likelihood[label] = likelihood[label]/(total_count + 2 * smoothing)\n",
    "        \n",
    "    return likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihood:\n",
      " {'Y': array([0.4, 0.6, 0.4]), 'N': array([0.33333333, 0.33333333, 0.66666667])}\n"
     ]
    }
   ],
   "source": [
    "# Setting smoothing = 1\n",
    "smoothing = 1\n",
    "likelihood = get_likelihood(X_train, label_indices, smoothing)\n",
    "print('Likelihood:\\n', likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posterior(X, prior, likelihood):\n",
    "    \"\"\"\n",
    "    Compute posteruor of testing samples, based on prior and likelihood\n",
    "    @param X: testing samples\n",
    "    @param prior: dictionary, with class label as key,\n",
    "                  corresponding prior as the value\n",
    "    @param likelihood: dictionary, with class label as key, \n",
    "                       corresponding conditional value as vector value\n",
    "    @return: dictionary, with class label as key, posterior as value\n",
    "    \"\"\"\n",
    "    posteriors = []\n",
    "    for x in X:\n",
    "        # posterior is proportional to prior * likelihood\n",
    "        posterior = prior.copy()\n",
    "        for label, likelihood_label in likelihood.items():\n",
    "            for index, bool_value in enumerate (x):\n",
    "                posterior[label] *= likelihood_label[index] if bool_value else (1 - likelihood_label[index])\n",
    "        # normalize so that all sums up to 1\n",
    "        sum_posterior = sum(posterior.values())\n",
    "        for label in posterior:\n",
    "            if posterior[label] == float('inf'):\n",
    "                posterior[label] = 1.0\n",
    "            else:\n",
    "                posterior[label] /= sum_posterior\n",
    "        posteriors.append(posterior.copy())\n",
    "    return posteriors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posterior:\n",
      " [{'Y': 0.9210360075805433, 'N': 0.07896399241945673}]\n"
     ]
    }
   ],
   "source": [
    "# From our example\n",
    "posterior = get_posterior(X_test,prior,likelihood)\n",
    "print('Posterior:\\n', posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that everything we have done above is what Naive Bayes does and we have done it from the scratch.\n",
    "\n",
    "Another possibility is to use *scikit-learn*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Naive Bayes with scikit-learn\n",
    "\n",
    "- We are going to use BernoulliNB module from scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-c7110ff39c25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnaive_bayes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBernoulliNB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[scikit-learn] Predicted probabilities:\n",
      " [[0.07896399 0.92103601]]\n",
      "[scikit-learn] Prediction: ['Y']\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model (alpha is the smoothing factor,\n",
    "#   fit_prior = True means prior learned from the training set)\n",
    "clf = BernoulliNB(alpha = 1.0, fit_prior=True)\n",
    "\n",
    "# Train the classifier\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "# Predict probability results\n",
    "pred_prob = clf.predict_proba(X_test)\n",
    "print('[scikit-learn] Predicted probabilities:\\n', pred_prob)\n",
    "\n",
    "# Or you can also get directly the predicted class by the method\n",
    "pred = clf.predict(X_test)\n",
    "print('[scikit-learn] Prediction:', pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Classification Performance\n",
    "\n",
    "To get more insights in the results, one can look not only to accuracy, but also to:\n",
    "\n",
    "1) **Confusion Matrix** (confusion_matrix from sklearn.metrics)\n",
    "\n",
    "\n",
    "|       |          | Predicted | Predicted|\n",
    "|-------|----------|:---------:|:--------:|\n",
    "|       |          | Negative  | Positive |\n",
    "|**Actual**| Negative |    TN     |    FP    |\n",
    "|**Actual**| Positive |    FN     |    TP    |\n",
    "\n",
    "Where:\n",
    "\n",
    "    - TN: True Negative\n",
    "    \n",
    "    - FP: False Positive\n",
    "\n",
    "    - FN: False Negative\n",
    "\n",
    "    - TP: True Positive\n",
    "\n",
    "1.1) Precision: $\\frac{TP}{TP + FP}$ - Fraction of positive cells measured correctly;\n",
    "\n",
    "1.2) Recall (True positive rate): $\\frac{TP}{TP + FN}$ - Fraction of positive cells correctly identified;\n",
    "\n",
    "1.3) F1 score (harmonic mean): $f_1 = 2 * \\frac{precision * recall}{precision + recall}$\n",
    "\n",
    "\n",
    "These measures can be obtained using scikit-learn by:\n",
    "\n",
    "`sklearn.metrics import precision_score, recall_score, f1_score`\n",
    "\n",
    "or a summarization of all is given by:\n",
    "\n",
    "`from sklearn.metrics import classification_report`\n",
    "\n",
    "\n",
    "\n",
    "2) Area under the curve (AUC) of the receiver operating characteristic (ROC):\n",
    "\n",
    "The distribution of True Positive and True Negative may overlap, if in some regions they do, then AUC is smaller than one. \n",
    "The closer the AUC is to one, the model is perfectly able to distinguish between positive class and negative class. \n",
    "\n",
    "https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning models with cross-validation\n",
    "\n",
    "We usually apply K-fold cross-validation. The idea of K-fold cross-validation is to divide the original dataset into *k* equal-sized subsets, which are retained as a testing set, the remaining k - 1 subsets are used to train model. Then the average performance across all k trials is calculated to generate overall result.\n",
    "\n",
    "Also, cross-validation is used to adjust hyperparameters, boosting learning perfomance and reducing overfitting.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3: Recognizing Faces with Support Vector Machine (SVM)\n",
    "\n",
    "## Finding Separating Boundary with SVM\n",
    "\n",
    "- The whole idea here is to find an optimal hyperplane in order to divide an N-dimensional space of features;\n",
    "\n",
    "- The hyprplane has the characteristic of being in (N-1)-dimensional space, such that if we talk about a 3D space, the hyperplane will be a 2D plane, if we are in a 2D space, the hyperplane will be a line;\n",
    "\n",
    "- Thus the idea is to find the hyperplane that maximizes the distance between the nearest features (points) in the N-dimensional space to the hyperplane. The nearest points delivering this maximization are caled **support vectors**.\n",
    "\n",
    "\n",
    "## Scenario 1: Identifying a separating hyperplane\n",
    "\n",
    "*Definition*: Let $w$ be a n-dimensional vector and $b$ an intercept. A separating hyperplane is such that:\n",
    "\n",
    "- For any data point x from one class, it satisfies $wx + b > 0$\n",
    "\n",
    "- For any data point x from another class, it satisfies $wx + b < 0$\n",
    "\n",
    "\n",
    "## Scenario 2: Determining the optimal hyperplane\n",
    "\n",
    "*Definitions*: \n",
    "\n",
    "- a) The nearest point(s) on the positive side of the hyperplane can constitute parallel hyperplane, this is called **positive hyperplane**. Analogously, in the negative side there is the **negative hyperplane**. The perpendicular distance between the negative and positive hyperplane is called **margin** and a **decision** hyperplane is optimal (or maximum-margin) if maximizes this distance. \n",
    "\n",
    "- b) Mathematically:\n",
    "\n",
    "    - **positive hyperplane**: $w x^{(p)} + b = 1$;\n",
    "\n",
    "    - **negative hyperplane**:: $w x^{(n)} + b = -1$;\n",
    "\n",
    "    - Where $x^{(p)}$ is a point in the positive hyperplane and $x^{(n)}%$ in the negative hyperplane;\n",
    "\n",
    "    - Let $i \\in \\{p,n\\}$. The distance between a point and the decision hyperplane is given by:\n",
    "\n",
    "    $$ \\frac{w x^{(i)} + b}{||w||} = \\frac{1}{||w||} $$\n",
    "\n",
    "    - Margin is then: $\\frac{w x^{(p)} + b}{||w||} +\\frac{w x^{(n)} + b}{||w||} = \\frac{2}{||w||}$;\n",
    "\n",
    "    - Thus we need to minimize $||w||$ to maximize margin.\n",
    "\n",
    "**Optimization problem** - Let $i \\in \\{p,n\\}$:\n",
    "\n",
    "- Minimze $||w||$\n",
    "\n",
    "- Subject to $y^{(i)}(w x^{(i)} + b) \\geq 0$ for the training set $\\{(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)})\\}$;\n",
    "    \n",
    "\n",
    "After optimization,we can use the model as classifier, such that if:\n",
    "\n",
    "- $ \\boldsymbol{w} \\boldsymbol{x}' + b > 0 \\rightarrow 1$;\n",
    "\n",
    "- $ \\boldsymbol{w} \\boldsymbol{x}' + b < 0 \\rightarrow -1$;\n",
    "\n",
    "- $||w x' + b||$ is the confidence of prediction, since it states the distance between the hyperplane and the specific point;\n",
    "\n",
    "\n",
    "## Scenario 3: Handling outliers\n",
    "\n",
    "- Often we cannot separate all the points with a hyperplane, some of them end up in the \"other side\"\n",
    "\n",
    "*Definitions*: \n",
    "\n",
    "- **misclassification error (hinge loss)**: the misclassification measure for a point $i$ is given by:\n",
    "\n",
    "    - i) If misclassified: $\\zeta^{(i)} = 1 - y^{(i)}(w x^{(i)} + b)$;\n",
    "\n",
    "    - ii) Otherwise: $\\zeta^{(i)} = 0$;\n",
    "\n",
    "\n",
    "**Optimization problem** - Let $i \\in \\{p,n\\}$:\n",
    "\n",
    "- Minimze $||w|| + C \\frac{\\sum_{i=1}^m \\zeta^{(i)}}{m}$ \n",
    "\n",
    "- Subject to $y^{(i)}(w x^{(i)} + b) \\geq 0$ for the training set $\\{(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)})\\}$;\n",
    "\n",
    "- Where $C$ is the hyperparameter defining the penalty for misclassification. If high, it will penalize it a lot, making model prone to overfiting, whereas if low, it allows more misclassified points and  possibly lead to low variance but high bias; \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing SVM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data size: (569, 30)\n",
      "Onput data size: (569,)\n",
      "Label names: ['malignant' 'benign']\n",
      " 357 positve samples and 212 negative samples.\n"
     ]
    }
   ],
   "source": [
    "# Loading dataset and performing some basic analysis\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer_data = load_breast_cancer()\n",
    "X = cancer_data.data\n",
    "Y = cancer_data.target\n",
    "print('Input data size:', X.shape)\n",
    "print('Onput data size:', Y.shape)\n",
    "print('Label names:', cancer_data.target_names)\n",
    "n_pos = (Y == 1).sum()\n",
    "n_neg = (Y == 0).sum()\n",
    "print(f' {n_pos} positve samples and {n_neg} negative samples.') # Serves as imbalance check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting training and testing sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SVM classifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel='linear', C=1.0, random_state = 42)\n",
    "# Kernel: will be explained latter on\n",
    "# C: Penalty parameter (default = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear', random_state=42)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model to training set\n",
    "\n",
    "clf.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is: 95.8%\n"
     ]
    }
   ],
   "source": [
    "accuracy = clf.score(X_test, Y_test)\n",
    "print(f'The accuracy is: {accuracy*100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 4: Dealing with more than two classes\n",
    "\n",
    "Two typical approaches:\n",
    "\n",
    "1) **one-vc-rest** (or **one-vs-all**)\n",
    "\n",
    "- For a K-class problem run K different binary SVM classifiers, such that if data falls in $k^{th}$ class it assumes one and zero otherwise.\n",
    "\n",
    "- Pick the class delivering higher confidence (larger value of $w_k x' + b_i$):\n",
    "\n",
    "$$y' = argmax_i(w_i x' + b_i)$$\n",
    "\n",
    "2) **one-vs-one**\n",
    "\n",
    "- Pairwise comparisons: Pick two classes (e.g $i$ and $j$) and trains a model on observations from $i$ (as the \"positive case\").\n",
    "Assign class to a new sample. Repeat this process for all paiwise combinations, which results in $\\frac{K(K-1)}{2} different classifiers.\n",
    "\n",
    "- The class that gets the most votes looking to all pairwise classifications is the winner. \n",
    "\n",
    "**Performance**\n",
    "\n",
    "- Generally speaking both have similar accuracy.\n",
    "\n",
    "- one-vs-one tends to be more memory efficient because it trains many models on smaller subset of data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data size : (178, 13)\n",
      "Output data size : (178,)\n",
      "Label names: ['class_0' 'class_1' 'class_2']\n",
      "59 class0 samples,\n",
      "71 class1 samples,\n",
      "48 class2 samples.\n"
     ]
    }
   ],
   "source": [
    "# Get data\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "wine_data = load_wine()\n",
    "X = wine_data.data\n",
    "Y = wine_data.target\n",
    "print('Input data size :', X.shape)\n",
    "print('Output data size :', Y.shape)\n",
    "print('Label names:', wine_data.target_names)\n",
    "n_class0 = (Y == 0).sum()\n",
    "n_class1 = (Y == 1).sum()\n",
    "n_class2 = (Y == 2).sum()\n",
    "print(f'{n_class0} class0 samples,\\n{n_class1} class1 samples,\\n{n_class2} class2 samples.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training and test set\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y,\n",
    "random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear', random_state=42)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply SVM classifier to data, initializing SVC model and iftting against training set\n",
    "\n",
    "clf = SVC(kernel='linear', C=1.0, random_state=42)\n",
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is: 97.8%\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on test set\n",
    "\n",
    "accuracy = clf.score(X_test, Y_test)\n",
    "print(f'The accuracy is: {accuracy*100:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-36bbae99698a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Check perfornabce for individual classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# Check perfornabce for individual classes\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "pred = clf.predict(X_test)\n",
    "print(classification_report(Y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 5: Solving linearly non-separable problems with kernels\n",
    "\n",
    "- There are some classification problems that cannot be solved with linear techniques.\n",
    "\n",
    "- For these cases **SVM with kernels** were invented, they convert the original feature $x^{(i)}$ \n",
    "to a higher dimensional feature space using a function $\\Phi$, such that the transformed dataset $\\Phi(x^{(i)})$\n",
    "is linearly separable. \n",
    "\n",
    "- New observations then become $(\\Phi(x^{(i)}),y^{(i)})$.\n",
    "\n",
    "- Along the process of SVM quadratic optimization, the features are involved in the form of pairwise\n",
    "dot product $x^{(i)} \\cdot x^{(j)}$. It would be more efficient to implement the $\\Phi$ tranformation \n",
    "first on the two low dimensional vectors than on the product itself. A function that satisfies it is\n",
    "called **kernel function**:\n",
    "\n",
    "$$K(x^{(i)},x^{(j)}) = \\Phi(x^{(i)}) \\cdot \\Phi(x^{(j)})$$\n",
    "\n",
    "Several kernels could be used:\n",
    "\n",
    "(i) **Radial Basis Function (RBF)**\n",
    "\n",
    "$$ K(x^{(i)},x^{(j)}) = exp \\left(-\\frac{||x^{(i)} - x^{(j)}||}{2\\sigma^2} \\right) =  exp \\left(-\\gamma ||x^{(i)} - x^{(j)}|| \\right)$$\n",
    "\n",
    "Where $\\gamma = \\frac{1}{2\\sigma^2}$.\n",
    "\n",
    "(ii) **Polynomial** (of degree d)\n",
    "\n",
    "$$ K(x^{(i)},x^{(j)}) = \\left( x^{(i)} \\cdot x^{(j)} + \\gamma \\right)^d $$\n",
    "\n",
    "(iii) **Sigmoid** \n",
    "\n",
    "$$ K(x^{(i)},x^{(j)}) = tanh \\left( x^{(i)} \\cdot x^{(j)} + \\gamma \\right) $$\n",
    "\n",
    "- The parameter $\\gamma$ is the **kernel coefficient** such that a large one (small $\\sigma$)\n",
    "means low variance allowed, representing exact fit on training set which may lead to overfitting. \n",
    "On the other hand, a small $\\gamma$ may lead to loose fit on training set, possibly causing underfitting. \n",
    "\n",
    "- Generally speaking, RBF kernel is preferrable over the others since there is no other parameter to tweak \n",
    "(as $d$ in polynomial) and sigmoid tends to perform similar to RBF (under certain parameters).\n",
    "\n",
    "### Choosing between linear and RBF kernels\n",
    "\n",
    "Three scenarios where linear kernel is favored over RBF:\n",
    "\n",
    "**Scenario 1**: Number of features and instances are high (over than 104 or 105). \n",
    "The dimension of feature space is too high and RBF transformation will not provide \n",
    "performance improvement but will increase computational expense. \n",
    "\n",
    "**Scenario 2**: Number of features in too large when compared to number of training samples.\n",
    "RBF kernel is more prone to overfitting.\n",
    "\n",
    "**Scenario 3**: Number of instances in too large when compared to number of features.\n",
    "RBF kernel for low dimension, generally boost performance, mapping it to higher-dimensional\n",
    "space. Due to training complexity, it becomes inefficient in a training set with more thant\n",
    "106 or 107 samples. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4: Pedicting Online Ad Click-Through with Tree-Based Algorithms\n",
    "\n",
    "- We will study Ad Click-Through prediction using tree based Algorithms;\n",
    "\n",
    "- For this chapter, scickit-learn and XGBoost will be used;\n",
    "\n",
    "### A Brief overview of ad click-though prediction\n",
    "\n",
    "- Industry had been interested in effectiviness of targeting. E.g. how likely is that a group of certain age will be interested in that product. \n",
    "\n",
    "- Most common measure of effectiviness is **click-thorugh rate (CTR)**, which is a ratio of total number of clicks to its total number of views.\n",
    "\n",
    "### Getting started with two types of data - numerical and categorical\n",
    "\n",
    "- Categorical features (also called qualitative) represent characteristics, distinct groups and a countable number of options.\n",
    "\n",
    "    - It may or may not have a logical order (in case there is, it is called ordinal);\n",
    "\n",
    "- Numerical (or quantitative) features have mathematical meaning as a measurament and are, by definition, ordered. \n",
    "\n",
    "### Exploring a decesion tree from the root to the leaves\n",
    "\n",
    "A decision tree is a diagram which contains a starting point and all possible alternatives. \n",
    "\n",
    "The starting point is called **root**, internal **node** represent basis on which a decision is made and a **terminal node** is called **leaf**.\n",
    "\n",
    "After a decision tree has been constructed, classify a new sample only requires to follow branches according to characteristics.\n",
    "\n",
    "### Constructing a decision tree\n",
    "\n",
    "The idea to build a decision tree is to partition training samples into successive subsets, when no partition can improve the purity of the subset, the partitioning ends. \n",
    "\n",
    "Many algorithms have been developed to contruct a tree in an efficient way. The idea behind them is to split based on optimal value of a feature.\n",
    "\n",
    "We will focus on CART (Calssification and Regression Tree) algorithm, which construct a tree through binary splitting.\n",
    "\n",
    "In each partition it will find the most significant combination of the feature and its value, dividing the dataset in two. One of them will be the sample whose categorical feature is equal to the specific value, or grate than this value (in case of numerical feature). The another will be the remaining one. \n",
    "\n",
    "It stops partitioning when one of the two criteria below is met:\n",
    "\n",
    "(i) The minimum number of samples for a new node: The number of samples is not greater than the minimum requirement for further split; \n",
    "\n",
    "(ii) The maximum depth of a tree: The depth of a tree is defined as the number of splits that occurred from the root node to terminal node. If it reaches the maximum tree depth, it stops. \n",
    "\n",
    "Both options are used to prevent overfitting. \n",
    "\n",
    "## The metrics for measuring a split\n",
    "\n",
    "### Gini Impurity\n",
    "\n",
    "- It measures the impurity rate of a class distribution of data points.\n",
    "\n",
    "For a dataset with K classes:\n",
    "\n",
    "$$Gini Impurity = 1 - \\sum_{k=1}^K f_k^2$$\n",
    "\n",
    "Where $f_k$ is the fraction of class $k$ in the dataset. \n",
    "\n",
    "Lower Gini implies a purer dataset. E.g a dataset in which all points belong to the same class has $K = 1$ and $f_k = 1$, such that $Gini Impurity= 0$.\n",
    "\n",
    "After a split a weighted (by sample) Gini Impurity measure is calculated to decide if that split is a good one. \n",
    "\n",
    "For binary cases, under different fraction of positve class variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"265.995469pt\" version=\"1.1\" viewBox=\"0 0 385.78125 265.995469\" width=\"385.78125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 265.995469 \nL 385.78125 265.995469 \nL 385.78125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 43.78125 228.439219 \nL 378.58125 228.439219 \nL 378.58125 10.999219 \nL 43.78125 10.999219 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m0bfabd857b\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"58.999432\" xlink:href=\"#m0bfabd857b\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0.0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n      </defs>\n      <g transform=\"translate(51.047869 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"119.872159\" xlink:href=\"#m0bfabd857b\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 0.2 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(111.920597 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"180.744886\" xlink:href=\"#m0bfabd857b\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 0.4 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(172.793324 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"241.617614\" xlink:href=\"#m0bfabd857b\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 0.6 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(233.666051 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"302.490341\" xlink:href=\"#m0bfabd857b\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 0.8 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g transform=\"translate(294.538778 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"363.363068\" xlink:href=\"#m0bfabd857b\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 1.0 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(355.411506 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- Positive Fraction -->\n     <defs>\n      <path d=\"M 19.671875 64.796875 \nL 19.671875 37.40625 \nL 32.078125 37.40625 \nQ 38.96875 37.40625 42.71875 40.96875 \nQ 46.484375 44.53125 46.484375 51.125 \nQ 46.484375 57.671875 42.71875 61.234375 \nQ 38.96875 64.796875 32.078125 64.796875 \nz\nM 9.8125 72.90625 \nL 32.078125 72.90625 \nQ 44.34375 72.90625 50.609375 67.359375 \nQ 56.890625 61.8125 56.890625 51.125 \nQ 56.890625 40.328125 50.609375 34.8125 \nQ 44.34375 29.296875 32.078125 29.296875 \nL 19.671875 29.296875 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-80\"/>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n      <path d=\"M 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 8.796875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nL 35.6875 0 \nL 23.484375 0 \nz\n\" id=\"DejaVuSans-118\"/>\n      <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n      <path id=\"DejaVuSans-32\"/>\n      <path d=\"M 9.8125 72.90625 \nL 51.703125 72.90625 \nL 51.703125 64.59375 \nL 19.671875 64.59375 \nL 19.671875 43.109375 \nL 48.578125 43.109375 \nL 48.578125 34.8125 \nL 19.671875 34.8125 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-70\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n     </defs>\n     <g transform=\"translate(169.825 256.715781)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-80\"/>\n      <use x=\"60.255859\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"121.4375\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"173.537109\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"201.320312\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"240.529297\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"268.3125\" xlink:href=\"#DejaVuSans-118\"/>\n      <use x=\"327.492188\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"389.015625\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"420.802734\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"478.212891\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"519.326172\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"580.605469\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"635.585938\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"674.794922\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"702.578125\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"763.759766\" xlink:href=\"#DejaVuSans-110\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mf9ef251e1c\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#mf9ef251e1c\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.0 -->\n      <g transform=\"translate(20.878125 232.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#mf9ef251e1c\" y=\"184.951219\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.2 -->\n      <g transform=\"translate(20.878125 188.750437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#mf9ef251e1c\" y=\"141.463219\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.4 -->\n      <g transform=\"translate(20.878125 145.262437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#mf9ef251e1c\" y=\"97.975219\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.6 -->\n      <g transform=\"translate(20.878125 101.774437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#mf9ef251e1c\" y=\"54.487219\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.8 -->\n      <g transform=\"translate(20.878125 58.286437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#mf9ef251e1c\" y=\"10.999219\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 1.0 -->\n      <g transform=\"translate(20.878125 14.798437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_14\">\n     <!-- Gini Impurity -->\n     <defs>\n      <path d=\"M 59.515625 10.40625 \nL 59.515625 29.984375 \nL 43.40625 29.984375 \nL 43.40625 38.09375 \nL 69.28125 38.09375 \nL 69.28125 6.78125 \nQ 63.578125 2.734375 56.6875 0.65625 \nQ 49.8125 -1.421875 42 -1.421875 \nQ 24.90625 -1.421875 15.25 8.5625 \nQ 5.609375 18.5625 5.609375 36.375 \nQ 5.609375 54.25 15.25 64.234375 \nQ 24.90625 74.21875 42 74.21875 \nQ 49.125 74.21875 55.546875 72.453125 \nQ 61.96875 70.703125 67.390625 67.28125 \nL 67.390625 56.78125 \nQ 61.921875 61.421875 55.765625 63.765625 \nQ 49.609375 66.109375 42.828125 66.109375 \nQ 29.4375 66.109375 22.71875 58.640625 \nQ 16.015625 51.171875 16.015625 36.375 \nQ 16.015625 21.625 22.71875 14.15625 \nQ 29.4375 6.6875 42.828125 6.6875 \nQ 48.046875 6.6875 52.140625 7.59375 \nQ 56.25 8.5 59.515625 10.40625 \nz\n\" id=\"DejaVuSans-71\"/>\n      <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-73\"/>\n      <path d=\"M 52 44.1875 \nQ 55.375 50.25 60.0625 53.125 \nQ 64.75 56 71.09375 56 \nQ 79.640625 56 84.28125 50.015625 \nQ 88.921875 44.046875 88.921875 33.015625 \nL 88.921875 0 \nL 79.890625 0 \nL 79.890625 32.71875 \nQ 79.890625 40.578125 77.09375 44.375 \nQ 74.3125 48.1875 68.609375 48.1875 \nQ 61.625 48.1875 57.5625 43.546875 \nQ 53.515625 38.921875 53.515625 30.90625 \nL 53.515625 0 \nL 44.484375 0 \nL 44.484375 32.71875 \nQ 44.484375 40.625 41.703125 44.40625 \nQ 38.921875 48.1875 33.109375 48.1875 \nQ 26.21875 48.1875 22.15625 43.53125 \nQ 18.109375 38.875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.1875 51.21875 25.484375 53.609375 \nQ 29.78125 56 35.6875 56 \nQ 41.65625 56 45.828125 52.96875 \nQ 50 49.953125 52 44.1875 \nz\n\" id=\"DejaVuSans-109\"/>\n      <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n      <path d=\"M 8.5 21.578125 \nL 8.5 54.6875 \nL 17.484375 54.6875 \nL 17.484375 21.921875 \nQ 17.484375 14.15625 20.5 10.265625 \nQ 23.53125 6.390625 29.59375 6.390625 \nQ 36.859375 6.390625 41.078125 11.03125 \nQ 45.3125 15.671875 45.3125 23.6875 \nL 45.3125 54.6875 \nL 54.296875 54.6875 \nL 54.296875 0 \nL 45.3125 0 \nL 45.3125 8.40625 \nQ 42.046875 3.421875 37.71875 1 \nQ 33.40625 -1.421875 27.6875 -1.421875 \nQ 18.265625 -1.421875 13.375 4.4375 \nQ 8.5 10.296875 8.5 21.578125 \nz\nM 31.109375 56 \nz\n\" id=\"DejaVuSans-117\"/>\n      <path d=\"M 32.171875 -5.078125 \nQ 28.375 -14.84375 24.75 -17.8125 \nQ 21.140625 -20.796875 15.09375 -20.796875 \nL 7.90625 -20.796875 \nL 7.90625 -13.28125 \nL 13.1875 -13.28125 \nQ 16.890625 -13.28125 18.9375 -11.515625 \nQ 21 -9.765625 23.484375 -3.21875 \nL 25.09375 0.875 \nL 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 11.921875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nz\n\" id=\"DejaVuSans-121\"/>\n     </defs>\n     <g transform=\"translate(14.798438 152.181719)rotate(-90)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-71\"/>\n      <use x=\"77.490234\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"105.273438\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"168.652344\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"196.435547\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"228.222656\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"257.714844\" xlink:href=\"#DejaVuSans-109\"/>\n      <use x=\"355.126953\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"418.603516\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"481.982422\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"523.095703\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"550.878906\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"590.087891\" xlink:href=\"#DejaVuSans-121\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_13\">\n    <path clip-path=\"url(#p07f7af00bd)\" d=\"M 58.999432 228.439219 \nL 65.397466 219.489763 \nL 71.795501 210.92464 \nL 77.888867 203.124696 \nL 83.982233 195.673353 \nL 90.075599 188.570611 \nL 96.168965 181.816469 \nL 101.957663 175.722926 \nL 107.746361 169.943996 \nL 113.535058 164.479677 \nL 119.323756 159.329971 \nL 124.807786 154.741512 \nL 130.291815 150.43542 \nL 135.775845 146.411695 \nL 141.259874 142.670336 \nL 146.439235 139.396103 \nL 151.618596 136.373733 \nL 156.797958 133.603228 \nL 161.977319 131.084587 \nL 167.15668 128.81781 \nL 172.336041 126.802897 \nL 177.210734 125.136585 \nL 182.085427 123.693377 \nL 186.96012 122.473274 \nL 191.834813 121.476276 \nL 196.709506 120.702382 \nL 201.584198 120.151593 \nL 206.458891 119.823908 \nL 211.333584 119.719328 \nL 216.208277 119.837852 \nL 221.08297 120.179481 \nL 225.957663 120.744214 \nL 230.832356 121.532052 \nL 235.707049 122.542995 \nL 240.581741 123.777042 \nL 245.456434 125.234193 \nL 250.331127 126.914449 \nL 255.20582 128.81781 \nL 260.385181 131.084587 \nL 265.564542 133.603228 \nL 270.743904 136.373733 \nL 275.923265 139.396103 \nL 281.102626 142.670336 \nL 286.281987 146.196434 \nL 291.766017 150.204472 \nL 297.250046 154.494877 \nL 302.734076 159.067649 \nL 308.218105 163.922787 \nL 314.006803 169.353989 \nL 319.795501 175.099802 \nL 325.584198 181.160228 \nL 331.372896 187.535266 \nL 337.466262 194.585718 \nL 343.559628 201.984772 \nL 349.652994 209.732425 \nL 355.746361 217.82868 \nL 362.144395 226.70493 \nL 363.363068 228.439219 \nL 363.363068 228.439219 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 43.78125 228.439219 \nL 43.78125 10.999219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 378.58125 228.439219 \nL 378.58125 10.999219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 43.78125 228.439219 \nL 378.58125 228.439219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 43.78125 10.999219 \nL 378.58125 10.999219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p07f7af00bd\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"43.78125\" y=\"10.999219\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "# Fraction of positive class varies from 0 to 1:\n",
    "\n",
    "pos_fraction = np.linspace(0.00, 1.00, 1000)\n",
    " \n",
    "# Gini impurity is calculated as\n",
    "\n",
    "gini = 1 - pos_fraction**2 - (1-pos_fraction)**2\n",
    "\n",
    "# We plot the results:\n",
    "\n",
    "plt.plot(pos_fraction,gini)\n",
    "plt.ylim(0,1)\n",
    "plt.xlabel('Positive Fraction')\n",
    "plt.ylabel('Gini Impurity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For binary cases, the highest impurity is achieved with positive fraction of 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.4800\n0.3200\n"
     ]
    }
   ],
   "source": [
    "# We can define a function for Gini Impurity\n",
    "\n",
    "def gini_impurity(labels):\n",
    "    # If set is empty, it is pure\n",
    "    if not labels:\n",
    "        return 0\n",
    "    # Count occurency of each label\n",
    "    counts = np.unique(labels, return_counts= True)[1]\n",
    "    fractions = counts / float(len(labels))\n",
    "    return 1 - np.sum(fractions ** 2)\n",
    "\n",
    "# Testing it\n",
    "print(f'{gini_impurity([1,1,1,0,0]):.4f}')\n",
    "\n",
    "print(f'{gini_impurity([1,1,1,1,0]):.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Gain\n",
    "\n",
    "Higher information gain implies better splitting..\n",
    "\n",
    "Information gain is calculate by comparison of entropy before and after the splitting.\n",
    "\n",
    "*Definition*: **Entropy** is a probabilistic measure of uncertainty. In a K-class dataset with $f_k$ denoting the fraction of each class in the entire dataset:\n",
    "\n",
    "$$ Entropy = - \\sum_{k=1}^K f_k * log_2 f_k $$\n",
    "\n",
    "Lower entropy means a purer dataset, such that if the fraction of one of two classes is 1, then entropy is zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-19-23b85a40c03d>:3: RuntimeWarning: divide by zero encountered in log2\n  ent = - (pos_fraction * np.log2(pos_fraction) +\n<ipython-input-19-23b85a40c03d>:3: RuntimeWarning: invalid value encountered in multiply\n  ent = - (pos_fraction * np.log2(pos_fraction) +\n<ipython-input-19-23b85a40c03d>:4: RuntimeWarning: divide by zero encountered in log2\n  (1 - pos_fraction) * np.log2(1 - pos_fraction))\n<ipython-input-19-23b85a40c03d>:4: RuntimeWarning: invalid value encountered in multiply\n  (1 - pos_fraction) * np.log2(1 - pos_fraction))\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"265.995469pt\" version=\"1.1\" viewBox=\"0 0 385.78125 265.995469\" width=\"385.78125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 265.995469 \nL 385.78125 265.995469 \nL 385.78125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 43.78125 228.439219 \nL 378.58125 228.439219 \nL 378.58125 10.999219 \nL 43.78125 10.999219 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"maa421f0581\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"58.694152\" xlink:href=\"#maa421f0581\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0.0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n      </defs>\n      <g transform=\"translate(50.74259 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"119.688991\" xlink:href=\"#maa421f0581\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 0.2 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(111.737429 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"180.68383\" xlink:href=\"#maa421f0581\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 0.4 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(172.732268 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"241.67867\" xlink:href=\"#maa421f0581\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 0.6 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(233.727107 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"302.673509\" xlink:href=\"#maa421f0581\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 0.8 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g transform=\"translate(294.721946 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"363.668348\" xlink:href=\"#maa421f0581\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 1.0 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(355.716785 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- Positive fraction -->\n     <defs>\n      <path d=\"M 19.671875 64.796875 \nL 19.671875 37.40625 \nL 32.078125 37.40625 \nQ 38.96875 37.40625 42.71875 40.96875 \nQ 46.484375 44.53125 46.484375 51.125 \nQ 46.484375 57.671875 42.71875 61.234375 \nQ 38.96875 64.796875 32.078125 64.796875 \nz\nM 9.8125 72.90625 \nL 32.078125 72.90625 \nQ 44.34375 72.90625 50.609375 67.359375 \nQ 56.890625 61.8125 56.890625 51.125 \nQ 56.890625 40.328125 50.609375 34.8125 \nQ 44.34375 29.296875 32.078125 29.296875 \nL 19.671875 29.296875 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-80\"/>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n      <path d=\"M 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 8.796875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nL 35.6875 0 \nL 23.484375 0 \nz\n\" id=\"DejaVuSans-118\"/>\n      <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n      <path id=\"DejaVuSans-32\"/>\n      <path d=\"M 37.109375 75.984375 \nL 37.109375 68.5 \nL 28.515625 68.5 \nQ 23.6875 68.5 21.796875 66.546875 \nQ 19.921875 64.59375 19.921875 59.515625 \nL 19.921875 54.6875 \nL 34.71875 54.6875 \nL 34.71875 47.703125 \nL 19.921875 47.703125 \nL 19.921875 0 \nL 10.890625 0 \nL 10.890625 47.703125 \nL 2.296875 47.703125 \nL 2.296875 54.6875 \nL 10.890625 54.6875 \nL 10.890625 58.5 \nQ 10.890625 67.625 15.140625 71.796875 \nQ 19.390625 75.984375 28.609375 75.984375 \nz\n\" id=\"DejaVuSans-102\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n     </defs>\n     <g transform=\"translate(170.935156 256.715781)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-80\"/>\n      <use x=\"60.255859\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"121.4375\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"173.537109\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"201.320312\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"240.529297\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"268.3125\" xlink:href=\"#DejaVuSans-118\"/>\n      <use x=\"327.492188\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"389.015625\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"420.802734\" xlink:href=\"#DejaVuSans-102\"/>\n      <use x=\"456.007812\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"497.121094\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"558.400391\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"613.380859\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"652.589844\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"680.373047\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"741.554688\" xlink:href=\"#DejaVuSans-110\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m4ee2d76f6f\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m4ee2d76f6f\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.0 -->\n      <g transform=\"translate(20.878125 232.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m4ee2d76f6f\" y=\"184.951219\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.2 -->\n      <g transform=\"translate(20.878125 188.750437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m4ee2d76f6f\" y=\"141.463219\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.4 -->\n      <g transform=\"translate(20.878125 145.262437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m4ee2d76f6f\" y=\"97.975219\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.6 -->\n      <g transform=\"translate(20.878125 101.774437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m4ee2d76f6f\" y=\"54.487219\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.8 -->\n      <g transform=\"translate(20.878125 58.286437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m4ee2d76f6f\" y=\"10.999219\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 1.0 -->\n      <g transform=\"translate(20.878125 14.798437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_14\">\n     <!-- Entropy -->\n     <defs>\n      <path d=\"M 9.8125 72.90625 \nL 55.90625 72.90625 \nL 55.90625 64.59375 \nL 19.671875 64.59375 \nL 19.671875 43.015625 \nL 54.390625 43.015625 \nL 54.390625 34.71875 \nL 19.671875 34.71875 \nL 19.671875 8.296875 \nL 56.78125 8.296875 \nL 56.78125 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-69\"/>\n      <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n      <path d=\"M 32.171875 -5.078125 \nQ 28.375 -14.84375 24.75 -17.8125 \nQ 21.140625 -20.796875 15.09375 -20.796875 \nL 7.90625 -20.796875 \nL 7.90625 -13.28125 \nL 13.1875 -13.28125 \nQ 16.890625 -13.28125 18.9375 -11.515625 \nQ 21 -9.765625 23.484375 -3.21875 \nL 25.09375 0.875 \nL 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 11.921875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nz\n\" id=\"DejaVuSans-121\"/>\n     </defs>\n     <g transform=\"translate(14.798438 139.254375)rotate(-90)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"63.183594\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"126.5625\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"165.771484\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"206.853516\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"268.035156\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"331.511719\" xlink:href=\"#DejaVuSans-121\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_13\">\n    <path clip-path=\"url(#p072fb87c6d)\" d=\"M 58.999432 225.956547 \nL 59.91527 220.251683 \nL 61.441668 212.316155 \nL 63.273344 203.987798 \nL 65.410301 195.247467 \nL 67.852537 186.137912 \nL 70.600052 176.717041 \nL 73.652847 167.046433 \nL 77.010921 157.187764 \nL 80.368995 148.006418 \nL 84.032349 138.647254 \nL 87.695702 129.884166 \nL 91.664336 120.984081 \nL 95.632969 112.636126 \nL 99.601602 104.788044 \nL 103.875515 96.84745 \nL 108.149427 89.39566 \nL 112.42334 82.397798 \nL 116.697253 75.82479 \nL 120.971165 69.652087 \nL 125.245078 63.858741 \nL 129.51899 58.426724 \nL 133.792903 53.340402 \nL 138.066816 48.586146 \nL 142.340728 44.152011 \nL 146.614641 40.027494 \nL 150.888554 36.203334 \nL 155.162466 32.671352 \nL 159.436379 29.42432 \nL 163.710292 26.455854 \nL 167.984204 23.760324 \nL 172.258117 21.33278 \nL 176.53203 19.168892 \nL 180.805942 17.264898 \nL 185.079855 15.617559 \nL 189.353768 14.224131 \nL 193.322401 13.155582 \nL 197.291034 12.302499 \nL 201.259667 11.663706 \nL 205.2283 11.238326 \nL 209.196933 11.02578 \nL 213.165567 11.02578 \nL 217.1342 11.238326 \nL 221.102833 11.663706 \nL 225.071466 12.302499 \nL 229.040099 13.155582 \nL 233.008732 14.224131 \nL 236.977366 15.509638 \nL 240.945999 17.013919 \nL 245.219911 18.881062 \nL 249.493824 21.007605 \nL 253.767737 23.397232 \nL 258.041649 26.054191 \nL 262.315562 28.983338 \nL 266.589475 32.190202 \nL 270.863387 35.681055 \nL 275.1373 39.462998 \nL 279.411213 43.544067 \nL 283.685125 47.933359 \nL 287.959038 52.641188 \nL 292.232951 57.679279 \nL 296.506863 63.061005 \nL 300.780776 68.801689 \nL 305.054688 74.918991 \nL 309.328601 81.433402 \nL 313.602514 88.368908 \nL 317.876426 95.753867 \nL 321.84506 103.043344 \nL 325.813693 110.782331 \nL 329.782326 119.010726 \nL 333.750959 127.778078 \nL 337.414313 136.403616 \nL 341.077667 145.606193 \nL 344.435741 154.621383 \nL 347.793815 164.283375 \nL 350.84661 173.736144 \nL 353.594125 182.911322 \nL 356.34164 192.893443 \nL 358.783876 202.6845 \nL 360.615553 210.857132 \nL 362.14195 218.555938 \nL 363.363068 225.956547 \nL 363.363068 225.956547 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 43.78125 228.439219 \nL 43.78125 10.999219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 378.58125 228.439219 \nL 378.58125 10.999219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 43.78125 228.439219 \nL 378.58125 228.439219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 43.78125 10.999219 \nL 378.58125 10.999219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p072fb87c6d\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"43.78125\" y=\"10.999219\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "# Calculating entropy for different fractions fo positve classs\n",
    "pos_fraction = np.linspace(0.00, 1.00, 1000)\n",
    "ent = - (pos_fraction * np.log2(pos_fraction) +\n",
    "(1 - pos_fraction) * np.log2(1 - pos_fraction))\n",
    "plt.plot(pos_fraction, ent)\n",
    "plt.xlabel('Positive fraction')\n",
    "plt.ylabel('Entropy')\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now we can write the function\n",
    "def entropy(labels):\n",
    "    if not labels:\n",
    "        return 0\n",
    "    counts = np.unique(labels, return_counts=True)[1]\n",
    "    fractions = counts / float(len(labels))\n",
    "    return - np.sum(fractions * np.log2(fractions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9710\n1.0000\n"
     ]
    }
   ],
   "source": [
    "print(f'{entropy([1, 1, 0, 1, 0]):.4f}')\n",
    "print(f'{entropy([1, 1, 0, 1, 0, 0]):.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Splits are then based on information gain which is basically the entropy after the split and before.\n",
    "\n",
    "- The entropy after the split is a weighted average (by sample) of the entropy of the split samples. \n",
    "\n",
    "The choice of Gini or Information Gain has generally little impact on the performance of trained tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_function = {'gini': gini_impurity,\n",
    "                      'entropy': entropy}\n",
    "def weighted_impurity(groups, criterion='gini'):\n",
    "    \"\"\"\n",
    "    Calculate weighted impurity of children after a split\n",
    "    @param groups: list of children, and a child consists a\n",
    "    list of class labels\n",
    "    @param criterion: metric to measure the quality of a split,\n",
    "    'gini' for Gini Impurity or 'entropy' for\n",
    "    Information Gain\n",
    "    @return: float, weighted impurity\n",
    "    \"\"\"\n",
    "    total = sum(len(group) for group in groups)\n",
    "    weighted_sum = 0.0\n",
    "    for group in groups:\n",
    "        weighted_sum += len(group) / float(total) * criterion_function[criterion](group)\n",
    "    return weighted_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Entropy of #1 split:0.9510\nEntropy of #2 split: 0.5510\n"
     ]
    }
   ],
   "source": [
    "# Example \n",
    "children_1 = [[1, 0, 1], [0, 1]]\n",
    "children_2 = [[1, 1], [0, 0, 1]]\n",
    "print(f\"Entropy of #1 split:{weighted_impurity(children_1,'entropy'):.4f}\")\n",
    "print(f\"Entropy of #2 split: {weighted_impurity(children_2,'entropy'):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a decision tree from the scratch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For computational reasons, we define gini impurity funtion with NumPy arrays\n",
    "\n",
    "def gini_impurity_np(labels):\n",
    "    # When the set is empty, it is also pure \n",
    "    if labels.size == 0:\n",
    "        return 0\n",
    "    # Count occurrencies of each label \n",
    "    counts = np.unique(labels, return_counts= True)[1]\n",
    "    fractions = counts / float(len(labels))\n",
    "    return 1- np.sum(fractions ** 2)\n",
    "\n",
    "def entropy_np(labels):\n",
    "    # When the set is empty, it is also pure\n",
    "    if labels.size == 0:\n",
    "        return 0\n",
    "    counts = np.unique(labels, return_counts=True)[1]\n",
    "    fractions = counts / float(len(labels))\n",
    "    return - np.sum(fractions * np.log2(fractions))\n",
    "\n",
    "def weighted_impurity(groups, criterion='gini'):\n",
    "    \"\"\"\n",
    "    Calculate weighted impurity of children after a split\n",
    "    @param groups: list of children, and a child consists a list\n",
    "    of class labels\n",
    "    @param criterion: metric to measure the quality of a split,\n",
    "    'gini' for Gini Impurity or\n",
    "    'entropy' for Information Gain\n",
    "    @return: float, weighted impurity\n",
    "    \"\"\"\n",
    "    total = sum(len(group) for group in groups)\n",
    "    weighted_sum = 0.0\n",
    "    for group in groups:\n",
    "        weighted_sum += len(group) / float(total) * criterion_function_np[criterion](group)\n",
    "    return weighted_sum\n",
    "\n",
    "def split_node(X, y, index, value):\n",
    "    \"\"\"\n",
    "    Split dataset X, y based on a feature and a value\n",
    "    @param X: numpy.ndarray, dataset feature\n",
    "    @param y: numpy.ndarray, dataset target\n",
    "    @param index: int, index of the feature used for splitting\n",
    "    @param value: value of the feature used for splitting\n",
    "    @return: list, list, left and right child, a child is in\n",
    "    the format of [X, y]\n",
    "    \"\"\"\n",
    "    x_index = X[:, index]\n",
    "    # if this feature is numerical\n",
    "    if X[0, index].dtype.kind in ['i', 'f']:\n",
    "        mask = x_index >= value\n",
    "    # if this feature is categorical\n",
    "    else:\n",
    "        mask = x_index == value\n",
    "        # split into left and right child\n",
    "        left = [X[~mask, :], y[~mask]]\n",
    "        right = [X[mask, :], y[mask]]\n",
    "    return left, right\n",
    "\n",
    "def get_best_split(X, y, criterion):\n",
    "    \"\"\"\n",
    "    Obtain the best splitting point and resulting children for\n",
    "    the dataset X, y\n",
    "    @param X: numpy.ndarray, dataset feature\n",
    "    @param y: numpy.ndarray, dataset target\n",
    "    @param criterion: gini or entropy\n",
    "    @return: dict {index: index of the feature, value: feature\n",
    "    value, children: left and right children}\n",
    "    \"\"\"\n",
    "    best_index, best_value, best_score, children = None, None, 1, None\n",
    "    for index in range(len(X[0])):\n",
    "        for value in np.sort(np.unique(X[:, index])):\n",
    "            groups = split_node(X, y, index, value)\n",
    "            impurity = weighted_impurity([groups[0][1], groups[1][1]], criterion)\n",
    "            if impurity < best_score:\n",
    "                best_index, best_value, best_score, children = index, value, impurity, groups\n",
    "    return {'index': best_index, 'value': best_value,'children': children}\n",
    "\n",
    "# When stopping criterion is met, the process stops at a node and the major label is assigned to left node.sum\n",
    "\n",
    "def get_leaf(labels):\n",
    "    # Obtain the leaf as the majority of the labels\n",
    "    return np.bincount(labels).argmax()\n",
    "\n",
    "# Lastly, we need a function that links all together\n",
    "\n",
    "# - Assign leaf node is one of two childs is empty \n",
    "# - Assign lef node if reaches maximum depth allowed \n",
    "# - Assign leaf node if the node has not enough samples to a further split \n",
    "# - If none is met, continue splitting\n",
    "\n",
    "def split(node, max_depth, min_size, depth, criterion):\n",
    "    \"\"\"\n",
    "    Split children of a node to construct new nodes or assign\n",
    "    them terminals\n",
    "    @param node: dict, with children info\n",
    "    @param max_depth: int, maximal depth of the tree\n",
    "    @param min_size: int, minimal samples required to further\n",
    "    split a child\n",
    "    @param depth: int, current depth of the node\n",
    "    @param criterion: gini or entropy\n",
    "    \"\"\"\n",
    "    left, right = node['children']\n",
    "    del (node['children'])\n",
    "    if left[1].size == 0:\n",
    "        node['right'] = get_leaf(right[1])\n",
    "        return\n",
    "    if right[1].size == 0:\n",
    "        node['left'] = get_leaf(left[1])\n",
    "        return\n",
    "    # Check if the current depth exceeds the maximal depth\n",
    "    if depth >= max_depth:\n",
    "        node['left'], node['right'] = get_leaf(left[1]), get_leaf(right[1])\n",
    "        return\n",
    "    # Check if the left child has enough samples\n",
    "    if left[1].size <= min_size:\n",
    "        node['left'] = get_leaf(left[1])\n",
    "    else:\n",
    "        # It has enough samples, we further split it\n",
    "        result = get_best_split(left[0], left[1], criterion)\n",
    "        result_left, result_right = result['children']\n",
    "        if result_left[1].size == 0:\n",
    "            node['left'] = get_leaf(result_right[1])\n",
    "        elif result_right[1].size == 0:\n",
    "            node['left'] = get_leaf(result_left[1])\n",
    "        else:\n",
    "            node['left'] = result\n",
    "            split(node['left'], max_depth, min_size, depth + 1, criterion)\n",
    "    # Check if the right child has enough samples\n",
    "    if right[1].size <= min_size:\n",
    "        node['right'] = get_leaf(right[1])\n",
    "    else:\n",
    "        # It has enough samples, we further split it\n",
    "        result = get_best_split(right[0], right[1], criterion)\n",
    "        result_left, result_right = result['children']\n",
    "        if result_left[1].size == 0:\n",
    "            node['right'] = get_leaf(result_right[1])\n",
    "        elif result_right[1].size == 0:\n",
    "            node['right'] = get_leaf(result_left[1])\n",
    "        else:\n",
    "            node['right'] = result\n",
    "            split(node['right'], max_depth, min_size, depth + 1, criterion)\n",
    "\n",
    "# Entry point of construction\n",
    "\n",
    "def train_tree(X_train, y_train, max_depth, min_size,criterion='gini'):\n",
    "    \"\"\"\n",
    "    Construction of a tree starts here\n",
    "    @param X_train: list of training samples (feature)\n",
    "    @param y_train: list of training samples (target)\n",
    "    @param max_depth: int, maximal depth of the tree\n",
    "    @param min_size: int, minimal samples required to further\n",
    "    split a child\n",
    "    @param criterion: gini or entropy\n",
    "    \"\"\"\n",
    "    X = np.array(X_train)\n",
    "    y = np.array(y_train)\n",
    "    root = get_best_split(X, y, criterion)\n",
    "    split(root, max_depth, min_size, 1, criterion)\n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test it \n",
    "import numpy as np\n",
    "criterion_function_np = {'gini': gini_impurity_np,'entropy': entropy_np}\n",
    "X_train = [['tech', 'professional'],\n",
    "            ['fashion', 'student'],\n",
    "            ['fashion', 'professional'],\n",
    "            ['sports', 'student'],\n",
    "            ['tech', 'student'],\n",
    "            ['tech', 'retired'],\n",
    "            ['sports', 'professional']]\n",
    "y_train = [1, 0, 0, 0, 1, 0, 1]\n",
    "tree = train_tree(X_train, y_train, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'index': 0, 'value': 'fashion', 'left': {'index': 1, 'value': 'professional', 'left': 0, 'right': 1}, 'right': 0}\n"
     ]
    }
   ],
   "source": [
    "print(tree)"
   ]
  },
  {
   "source": [
    "### Implementing decision tree with scikit-learn\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                       max_depth=2, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier,export_graphviz\n",
    "X_train_n = [[6, 7],\n",
    "            [2, 4],\n",
    "            [7, 2],\n",
    "            [3, 6],\n",
    "            [4, 7],\n",
    "            [5, 2],\n",
    "            [1, 6],\n",
    "            [2, 0],\n",
    "            [6, 3],\n",
    "            [4, 1]]\n",
    "y_train_n = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
    "tree_sk = DecisionTreeClassifier(criterion='gini',max_depth=2, min_samples_split=2)\n",
    "tree_sk.fit(X_train_n, y_train_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_graphviz(tree_sk, out_file='tree.dot',\n",
    "                    feature_names=['X1', 'X2'], impurity=False,\n",
    "                    filled=True, class_names=['0', '1'])"
   ]
  },
  {
   "source": [
    "The command above has generated a .dot file containing the tree visualization. \n",
    "\n",
    "To visualize it, you can open the \"tree.dot\" file and run ctrl + shift + v (if you have installed https://marketplace.visualstudio.com/items?itemName=joaompinto.vscode-graphviz)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Ensembling decision trees - random forest\n",
    "\n",
    "- If one gets random samples from original dataset with replacement, each set fit an individual classification model and results are combined thorugh **majority vote** rule of decision, we can say she is using the technique of **bagging** (**booststrap aggregating**). \n",
    "\n",
    "- Generally speaking bagging reduces high variance of a tree and generally performs better than a single tree. However when few features are strong indicators, individual trees are highly based on these features, making trees highly correlated, reducing the benefits of aggregating multiple trees. To circumvent this problem, random forest also randomly select a subset of features when searching for the best split in each node. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Implementing random forest"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "random_forest = RandomForestClassifier(n_estimators=100,\n",
    "                                       criterion='gini', min_samples_split=30,\n",
    "                                       n_jobs=-1)"
   ]
  },
  {
   "source": [
    "Main hyperparameters:\n",
    "\n",
    "- max_depth: The maximum \"depth\" of an individual tree in terms of nodes (overfit if too deep and underfit if too shallow);\n",
    "\n",
    "- min_samples_split: The minimum number of samples required for a split (overfit if too small and underfit if too large);\n",
    "\n",
    "- max_features: Number o festures to consider for each best splitting point search. Typically square root of m number of total features is used. \n",
    "In scikit-learn this can be specified as `max_features = \"sqrt\"`;\n",
    "\n",
    "- n_estimators: The number of estimated trees (usually 100, 200, 500);\n",
    "\n",
    "To fine tune the hyperparameters we use the following:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score=nan,\n",
       "             estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                              class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              max_samples=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=30,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              n_estimators=100, n_jobs=-1,\n",
       "                                              oob_score=False,\n",
       "                                              random_state=None, verbose=0,\n",
       "                                              warm_start=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'max_depth': [3, 10, None]}, pre_dispatch='2*n_jobs',\n",
       "             refit=True, return_train_score=False, scoring='roc_auc',\n",
       "             verbose=0)"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'max_depth': [3, 10, None]}\n",
    "grid_search = GridSearchCV(random_forest, parameters,n_jobs=-1, cv=3, scoring='roc_auc')\n",
    "grid_search.fit(X_train_n, y_train_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'max_depth': 3}\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.best_params_)"
   ]
  },
  {
   "source": [
    "## Ensembling decion trees - gradient boosted trees"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**Boosting** is another ensemble technique, which instead of combining multiple learners in parallel, each tree is longer trained separately.\n",
    "\n",
    "In one of the boosting methods, called **Gradient boosted trees (BGT)** trees are trained successvely aiing to correct errors made by previous one. \n",
    "\n",
    "To implement it, you can use the XGBoost package (pip install xgboost)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Chapter 7: Predicting Stock Prices with Regression Algorithms"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}