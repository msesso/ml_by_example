{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning by Example (2020)\n",
    "\n",
    "\"*In traditional programming, the computer follows a set of predefined rules to\n",
    "process the input data and produce the outcome. In machine learning, the computer\n",
    "tries to mimic human thinking.*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks can be classified into:\n",
    "\n",
    "1) Unsupervised Learning: Data used for learning has indicative signals but no description. Ex: Anomalies detection;\n",
    "\n",
    "2) Supervise Learning: Goal is to find a function mapping inputs to output, so in this sense data comes with description, targets or desired output. Ex: Sales forecasting;\n",
    "\n",
    "3) Reinforcement Learning: System can adapt to certain dynamic conditions with data providing feedbacks. There is a goal in the end and the system understands its perfomance, adjusting accordingly. Ex: Self-driven cars;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting, underfitting, and the bias-variance trade-off\n",
    "\n",
    "Concepts recap:\n",
    "\n",
    "**Bias**: \n",
    "\n",
    "-> Error from incorrect assumptions iin learning algorithm:\n",
    "\n",
    "\\begin{align}\n",
    "Bias[ \\hat y ] = E[\\hat y - y ]\n",
    "\\end{align}\n",
    "\n",
    "**Variance**: \n",
    "\n",
    "-> Sensitivity of the model regarding variations in dataset:\n",
    "\n",
    "\\begin{align}\n",
    "Variance = E[ \\hat y^2 ] - E[\\hat y]^2\n",
    "\\end{align}\n",
    "\n",
    "**Mean Squared Error (MSE)**:\n",
    "\n",
    "-> A measure for the error of estimation\n",
    "\n",
    "\\begin{align}\n",
    "MSE = E[(y(x) - \\hat y (x))^2]\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "**Overfitting**: The model is fitting the training set extremely well, but it is not good for predictions, in this sense, it does not have \"external validity\".\n",
    "    \n",
    "- Its bias is low, but variance is high, since preadictions tend to have large variability;\n",
    "    \n",
    "**Underfitting**: The model perfoms badly in training and test sets.\n",
    "\n",
    "- Its bias is high, variance potentially low (in case our model is extremely simple, think about a a straight horizontal line as prediction);\n",
    "    \n",
    "**Bias-variance trade-off**\n",
    "\n",
    "More data and complex models tend to reduce bias, however there will be more shifts in the model to better fit the data, increasing variance.\n",
    "\n",
    "\\begin{align}\n",
    "MSE & = E[(y - \\hat y)^2]\\\\\n",
    "& = E \\left[(y-E[\\hat y] + E[\\hat y] - \\hat y)^2\\right]\\\\\n",
    "& = E \\left[(y-E[\\hat y])^2 \\right] + E\\left[(E[\\hat y] - \\hat y)^2\\right] + E\\left[2\\left(y-E[\\hat y]\\right)\\left(E[\\hat y] - \\hat y\\right)\\right]\\\\\n",
    "& = E \\left[(y-E[\\hat y])^2 \\right] + E\\left[(E[\\hat y] - \\hat y)^2\\right] + 2\\left(y-E[\\hat y]\\right)\\left(E[\\hat y] - E[\\hat y]\\right)\\\\\n",
    "& = \\left(E[\\hat y - y]\\right)^2 + E[\\hat y^2] - E[\\hat y ]^2\\\\\n",
    "& = \\underbrace{Bias[ \\hat y ]^2}_{\\text{Error of estimations}} + \\underbrace{Variance[ \\hat y ]}_{\\hat y \\text{ movement around its mean}}\n",
    "\\end{align}\n",
    "    \n",
    "**Cross-validation**\n",
    "\n",
    "Cross-validation helps in avoiding overfitting, such that the training set is split into training and validation set. \n",
    "\n",
    "It can be: (1) **exhaustive**: When all possible partitions are tested (e.g. Leave-One-Out-Cross_Validation - LOOCV); (2) **Non-exhaustive**: Not all possible partitions are used (e.g. k-fold cross validation - set is split in k equal-size folds leaving one out for test in each of the k rounds);\n",
    "\n"
   ]
  },
  {
   "source": [
    "## Chapter 2: Building a Movie Recommendation Engine with Naive Bayes\n",
    "\n",
    "- Movie recommendation is a classification problem.\n",
    "\n",
    "- Generally speaking classification maps observations/features/predictive variables to target categories/labels/classes.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Binary Classification\n",
    "\n",
    "- Classify observations in one of two possible classes (e.g. spam detection, click-thorugh for online ads, whether a person likes or not a movie).\n",
    "\n",
    "### Multiclass Classification\n",
    "\n",
    "- Classify observations in more than two possible classes (e.g. handwritten digit recognition as number 9, 2, etc).\n",
    "\n",
    "### Multi-label Classification\n",
    "\n",
    "- An observation can belong to more than one class (e.g. a movie can be classified as adventure, sci-fi).\n",
    "\n",
    "- Typical approach to solve is divide it in a set of binary problem classification.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Exploring Naive Bayes\n",
    "\n",
    "- Probabilistic classifier\n",
    "\n",
    "#### Recall Bayes' Theorem\n",
    "\n",
    "$$P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$$\n",
    "\n",
    "E.g. If I have a unfair coin (U) and a fair one (F), such that in the first one the probability of head (P(H|U)=90%), so given that we got head, what it the probability that an unfair coin was picked?\n",
    "\n",
    "Answer: $$P(U|H) = \\frac{P(H|U)P(U)}{P(H)} = \\frac{P(H|U)P(U)}{P(H|U)P(U) + P(H|F)P(F)} = \\frac{0.9*0.5}{0.9*0.5+0.5*0.5} = 0.64$$\n",
    "\n",
    "#### The mecanics of Naive Bayes\n",
    "\n",
    "Consider:\n",
    "\n",
    "Let $k \\in \\{1,2,...,K\\}$ denote classes, the probability that a sample belong to class $k$ given observed $x$ is: \n",
    "\n",
    "$$P(y_k|x) = \\frac{P(x|y_k)P(y_k)}{P(x)}$$\n",
    "\n",
    "The names given for the components of the equation above are:\n",
    "\n",
    "- **Prior**: $P(y_k)$ - How classes are distributed, without any knowledge of features;\n",
    "\n",
    "- **Posterior**: $P(y_k|x)$ - Incorporates knowledge from observation;\n",
    "\n",
    "- **Likelihood** - P(x|y_k) - The distribution of n features given that the sample belong to class $y_k$. \n",
    "Likelihood ends up being very hard to calculate when there are a large number of features, since this become a large joint distribution.\n",
    "To circumvent this issue, Naive Bayes assumes feature independence, which allow us to write:\n",
    "\n",
    "$$P(x|y_k) = P(x_1|y_k)*P(x_2|y_k)*...*P(x_n|y_k)$$\n",
    "\n",
    "The denominator of our bayes formula, $P(x)$ (called **evidence**) depends on overall distribution of fetures, meaning that it acts a constant, \n",
    "and so our posterior is proportional to:\n",
    "\n",
    "$$ P(y_k|x) \\propto P(x|y_k)P(y_k) = P(x_1|y_k)*P(x_2|y_k)*...*P(x_n|y_k) $$\n",
    "\n",
    "Note that it is possible that, for a given sample, a given feature, say $n'$ presents: $P(x_{n'}|y_k) = 0$, which would cause the likelihood to be zero, \n",
    "and so an unknown likelihood. To avoid that, **Laplace Smoothing** is used:\n",
    "\n",
    "$$ P(x_{n'}|y_k) = \\frac{N_{x_{n'}|y_k} + \\alpha}{N_{y_k}+ \\alpha d}$$\n",
    "\n",
    "Where $N_{x_{n'}|y_k}$ is how many times $x_{n'}$ occured given that $y_k$ was observed, $N_{y_k}$ how many times $y_k$ wa observed, $\\alpha>0$ is the smoothing parameter ($\\alpha=0$ means no \n",
    "smoothing, many times this is set to 1) and $d$ in the binary case is 2 (because there are two possible values).\n",
    "\n",
    "Knowing the likelihoods and given some prior, one can calculate the posteriors. Using the fact that the sum of posteriors for a given $x$ is 1, the probaility that\n",
    "$y_k$ is observed given $x$ is found. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Implementing Naive Bayes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Building a toy dataset, which tries to discover if the user likes the target movie based on how she likes other three movies (like is YES or NO)\n",
    "X_train = np.array([\n",
    "    [0, 1, 1],\n",
    "    [0, 0, 1],\n",
    "    [0, 0, 0],\n",
    "    [1, 1, 0]])\n",
    "Y_train = ['Y', 'N', 'Y', 'Y']\n",
    "X_test = np.array([[1, 1, 0]])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by label ('Y' and 'N') recording their indices (where they show up) by classes\n",
    "\n",
    "def get_label_indices(labels):\n",
    "    \"\"\"\n",
    "    Group samples based on their labels and return indices\n",
    "    @param labels: list of labels\n",
    "    @return: dict, {class1: [indices], class2: [indices]}\n",
    "    \"\"\"\n",
    "    from collections import defaultdict\n",
    "    label_indices = defaultdict(list)\n",
    "    for index, label in enumerate(labels):\n",
    "        #print('index:',index)\n",
    "        #print('label', label)\n",
    "        label_indices[label].append(index)\n",
    "    return label_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "label_indices:\n defaultdict(<class 'list'>, {'Y': [0, 2, 3], 'N': [1]})\n"
     ]
    }
   ],
   "source": [
    "label_indices = get_label_indices(Y_train)\n",
    "print('label_indices:\\n', label_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prior(label_indices):\n",
    "    \"\"\"\n",
    "    Compute prior based on training examples\n",
    "    @param label_indices: grouped sample indices by class\n",
    "    @return: dictionary, with class label as key, corresponding prior\n",
    "             as the value\n",
    "    \"\"\"\n",
    "    # define prior as an object that can be referred by label\n",
    "    #  get the length of indices inside label_indices items\n",
    "    prior = {label: len(indices) for label, indices in label_indices.items()}\n",
    "    total_count = sum(prior.values())\n",
    "    for label in prior:\n",
    "        prior[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_items([('Y', [0, 2, 3]), ('N', [1])])"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "label_indices.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}