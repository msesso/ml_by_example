{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning by Example (2020)\n",
    "\n",
    "\"*In traditional programming, the computer follows a set of predefined rules to\n",
    "process the input data and produce the outcome. In machine learning, the computer\n",
    "tries to mimic human thinking.*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks can be classified into:\n",
    "\n",
    "1) Unsupervised Learning: Data used for learning has indicative signals but no description. Ex: Anomalies detection;\n",
    "\n",
    "2) Supervise Learning: Goal is to find a function mapping inputs to output, so in this sense data comes with description, targets or desired output. Ex: Sales forecasting;\n",
    "\n",
    "3) Reinforcement Learning: System can adapt to certain dynamic conditions with data providing feedbacks. There is a goal in the end and the system understands its perfomance, adjusting accordingly. Ex: Self-driven cars;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting, underfitting, and the bias-variance trade-off\n",
    "\n",
    "Concepts recap:\n",
    "\n",
    "**Bias**: \n",
    "\n",
    "-> Error from incorrect assumptions iin learning algorithm:\n",
    "\n",
    "\\begin{align}\n",
    "Bias[ \\hat y ] = E[\\hat y - y ]\n",
    "\\end{align}\n",
    "\n",
    "**Variance**: \n",
    "\n",
    "-> Sensitivity of the model regarding variations in dataset:\n",
    "\n",
    "\\begin{align}\n",
    "Variance = E[ \\hat y^2 ] - E[\\hat y]^2\n",
    "\\end{align}\n",
    "\n",
    "**Mean Squared Error (MSE)**:\n",
    "\n",
    "-> A measure for the error of estimation\n",
    "\n",
    "\\begin{align}\n",
    "MSE = E[(y(x) - \\hat y (x))^2]\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "**Overfitting**: The model is fitting the training set extremely well, but it is not good for predictions, in this sense, it does not have \"external validity\".\n",
    "    \n",
    "- Its bias is low, but variance is high, since preadictions tend to have large variability;\n",
    "    \n",
    "**Underfitting**: The model perfoms badly in training and test sets.\n",
    "\n",
    "- Its bias is high, variance potentially low (in case our model is extremely simple, think about a a straight horizontal line as prediction);\n",
    "    \n",
    "**Bias-variance trade-off**\n",
    "\n",
    "More data and complex models tend to reduce bias, however there will be more shifts in the model to better fit the data, increasing variance.\n",
    "\n",
    "\\begin{align}\n",
    "MSE & = E[(y - \\hat y)^2]\\\\\n",
    "& = E \\left[(y-E[\\hat y] + E[\\hat y] - \\hat y)^2\\right]\\\\\n",
    "& = E \\left[(y-E[\\hat y])^2 \\right] + E\\left[(E[\\hat y] - \\hat y)^2\\right] + E\\left[2\\left(y-E[\\hat y]\\right)\\left(E[\\hat y] - \\hat y\\right)\\right]\\\\\n",
    "& = E \\left[(y-E[\\hat y])^2 \\right] + E\\left[(E[\\hat y] - \\hat y)^2\\right] + 2\\left(y-E[\\hat y]\\right)\\left(E[\\hat y] - E[\\hat y]\\right)\\\\\n",
    "& = \\left(E[\\hat y - y]\\right)^2 + E[\\hat y^2] - E[\\hat y ]^2\\\\\n",
    "& = \\underbrace{Bias[ \\hat y ]^2}_{\\text{Error of estimations}} + \\underbrace{Variance[ \\hat y ]}_{\\hat y \\text{ movement around its mean}}\n",
    "\\end{align}\n",
    "    \n",
    "**Cross-validation**\n",
    "\n",
    "Cross-validation helps in avoiding overfitting, such that the training set is split into training and validation set. \n",
    "\n",
    "It can be: (1) **exhaustive**: When all possible partitions are tested (e.g. Leave-One-Out-Cross_Validation - LOOCV); (2) **Non-exhaustive**: Not all possible partitions are used (e.g. k-fold cross validation - set is split in k equal-size folds leaving one out for test in each of the k rounds);\n",
    "\n"
   ]
  },
  {
   "source": [
    "## Chapter 2: Building a Movie Recommendation Engine with Naive Bayes\n",
    "\n",
    "- Movie recommendation is a classification problem.\n",
    "\n",
    "- Generally speaking classification maps observations/features/predictive variables to target categories/labels/classes.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Binary Classification\n",
    "\n",
    "- Classify observations in one of two possible classes (e.g. spam detection, click-thorugh for online ads, whether a person likes or not a movie).\n",
    "\n",
    "### Multiclass Classification\n",
    "\n",
    "- Classify observations in more than two possible classes (e.g. handwritten digit recognition as number 9, 2, etc).\n",
    "\n",
    "### Multi-label Classification\n",
    "\n",
    "- An observation can belong to more than one class (e.g. a movie can be classified as adventure, sci-fi).\n",
    "\n",
    "- Typical approach to solve is divide it in a set of binary problem classification.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Exploring Naive Bayes\n",
    "\n",
    "- Probabilistic classifier\n",
    "\n",
    "#### Recall Bayes' Theorem\n",
    "\n",
    "$$P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$$\n",
    "\n",
    "E.g. If I have a unfair coin (U) and a fair one (F), such that in the first one the probability of head (P(H|U)=90%), so given that we got head, what it the probability that an unfair coin was picked?\n",
    "\n",
    "Answer: $$P(U|H) = \\frac{P(H|U)P(U)}{P(H)} = \\frac{P(H|U)P(U)}{P(H|U)P(U) + P(H|F)P(F)} = \\frac{0.9*0.5}{0.9*0.5+0.5*0.5} = 0.64$$\n",
    "\n",
    "#### The mecanics of Naive Bayes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}