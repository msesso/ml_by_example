{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning by Example (2020)\n",
    "\n",
    "\"*In traditional programming, the computer follows a set of predefined rules to\n",
    "process the input data and produce the outcome. In machine learning, the computer\n",
    "tries to mimic human thinking.*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks can be classified into:\n",
    "\n",
    "1) Unsupervised Learning: Data used for learning has indicative signals but no description. Ex: Anomalies detection;\n",
    "\n",
    "2) Supervised Learning: Goal is to find a function mapping inputs to output, so in this sense data comes with description, targets or desired output. Ex: Sales forecasting;\n",
    "\n",
    "3) Reinforcement Learning: System can adapt to certain dynamic conditions with data providing feedbacks. There is a goal in the end and the system understands its perfomance, adjusting accordingly. Ex: Self-driven cars;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting, underfitting, and the bias-variance trade-off\n",
    "\n",
    "Concepts recap:\n",
    "\n",
    "**Bias**: \n",
    "\n",
    "-> Error from incorrect assumptions in learning algorithm:\n",
    "\n",
    "\\begin{align}\n",
    "Bias[ \\hat y ] = E[\\hat y - y ]\n",
    "\\end{align}\n",
    "\n",
    "**Variance**: \n",
    "\n",
    "-> Sensitivity of the model regarding variations in dataset:\n",
    "\n",
    "\\begin{align}\n",
    "Variance = E[ \\hat y^2 ] - E[\\hat y]^2\n",
    "\\end{align}\n",
    "\n",
    "**Mean Squared Error (MSE)**:\n",
    "\n",
    "-> A measure for the error of estimation\n",
    "\n",
    "\\begin{align}\n",
    "MSE = E[(y(x) - \\hat y (x))^2]\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "**Overfitting**: The model is fitting the training set extremely well, but it is not good for predictions, in this sense, it does not have \"external validity\".\n",
    "    \n",
    "- Its bias is low, but variance is high, since predictions tend to have large variability;\n",
    "    \n",
    "**Underfitting**: The model perfoms badly in training and test sets.\n",
    "\n",
    "- Its bias is high, variance potentially low (in case our model is extremely simple, think about a straight horizontal line as prediction);\n",
    "    \n",
    "**Bias-variance trade-off**\n",
    "\n",
    "More data and complex models tend to reduce bias, however there will be more shifts in the model to better fit the data, increasing variance.\n",
    "\n",
    "\\begin{align}\n",
    "MSE & = E[(y - \\hat y)^2]\\\\\n",
    "& = E \\left[(y-E[\\hat y] + E[\\hat y] - \\hat y)^2\\right]\\\\\n",
    "& = E \\left[(y-E[\\hat y])^2 \\right] + E\\left[(E[\\hat y] - \\hat y)^2\\right] + E\\left[2\\left(y-E[\\hat y]\\right)\\left(E[\\hat y] - \\hat y\\right)\\right]\\\\\n",
    "& = E \\left[(y-E[\\hat y])^2 \\right] + E\\left[(E[\\hat y] - \\hat y)^2\\right] + 2\\left(y-E[\\hat y]\\right)\\left(E[\\hat y] - E[\\hat y]\\right)\\\\\n",
    "& = \\left(E[\\hat y - y]\\right)^2 + E[\\hat y^2] - E[\\hat y ]^2\\\\\n",
    "& = \\underbrace{Bias[ \\hat y ]^2}_{\\text{Error of estimations}} + \\underbrace{Variance[ \\hat y ]}_{\\hat y \\text{ movement around its mean}}\n",
    "\\end{align}\n",
    "    \n",
    "**Cross-validation**\n",
    "\n",
    "Cross-validation helps in avoiding overfitting, such that the training set is split into training and validation set. \n",
    "\n",
    "It can be: (1) **exhaustive**: When all possible partitions are tested (e.g. Leave-One-Out-Cross_Validation - LOOCV); (2) **Non-exhaustive**: Not all possible partitions are used (e.g. k-fold cross validation - set is split in k equal-size folds leaving one out for test in each of the k rounds);\n",
    "\n"
   ]
  },
  {
   "source": [
    "## Chapter 2: Building a Movie Recommendation Engine with Naive Bayes\n",
    "\n",
    "- Movie recommendation is a classification problem.\n",
    "\n",
    "- Generally speaking classification maps observations/features/predictive variables to target categories/labels/classes.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Binary Classification\n",
    "\n",
    "- Classify observations in one of two possible classes (e.g. spam detection, click-thorugh for online ads, whether a person likes or not a movie).\n",
    "\n",
    "### Multiclass Classification\n",
    "\n",
    "- Classify observations in more than two possible classes (e.g. handwritten digit recognition as number 9, 2, etc).\n",
    "\n",
    "### Multi-label Classification\n",
    "\n",
    "- An observation can belong to more than one class (e.g. a movie can be classified as adventure, sci-fi).\n",
    "\n",
    "- Typical approach to solve is divide it in a set of binary problem classification.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Exploring Naive Bayes\n",
    "\n",
    "- Probabilistic classifier\n",
    "\n",
    "#### Recall Bayes' Theorem\n",
    "\n",
    "$$P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$$\n",
    "\n",
    "E.g. If I have a unfair coin (U) and a fair one (F), such that in the first one the probability of head is 90% (P(H|U)=90%). Given that we got head, what it the probability that an unfair coin was picked?\n",
    "\n",
    "Answer: $$P(U|H) = \\frac{P(H|U)P(U)}{P(H)} = \\frac{P(H|U)P(U)}{P(H|U)P(U) + P(H|F)P(F)} = \\frac{0.9*0.5}{0.9*0.5+0.5*0.5} = 0.64$$\n",
    "\n",
    "#### The mecanics of Naive Bayes\n",
    "\n",
    "Consider:\n",
    "\n",
    "Let $k \\in \\{1,2,...,K\\}$ denote classes, the probability that a sample belong to class $k$ given observed $x$ is: \n",
    "\n",
    "$$P(y_k|x) = \\frac{P(x|y_k)P(y_k)}{P(x)}$$\n",
    "\n",
    "The names given for the components of the equation above are:\n",
    "\n",
    "- **Prior**: $P(y_k)$ - How classes are distributed, without any knowledge of features;\n",
    "\n",
    "- **Posterior**: $P(y_k|x)$ - Incorporates knowledge from observation;\n",
    "\n",
    "- **Likelihood** - $P(x|y_k)$ - The distribution of n features given that the sample belong to class $y_k$. \n",
    "Likelihood ends up being very hard to calculate when there are a large number of features, since this become a large joint distribution.\n",
    "To circumvent this issue, Naive Bayes assumes feature independence, which allow us to write:\n",
    "\n",
    "$$P(x|y_k) = P(x_1|y_k)*P(x_2|y_k)*...*P(x_n|y_k)$$\n",
    "\n",
    "The denominator of our bayes formula, $P(x)$ (called **evidence**) depends on overall distribution of features, meaning that it acts a constant, \n",
    "and so our posterior is proportional to:\n",
    "\n",
    "$$ P(y_k|x) \\propto P(x|y_k)P(y_k) = P(x_1|y_k)*P(x_2|y_k)*...*P(x_n|y_k) $$\n",
    "\n",
    "Note that it is possible that, for a given sample, a given feature, say $n'$ presents: $P(x_{n'}|y_k) = 0$, which would cause the likelihood to be zero, \n",
    "and so an unknown likelihood. To avoid that, **Laplace Smoothing** is used:\n",
    "\n",
    "$$ P(x_{n'}|y_k) = \\frac{N_{x_{n'}|y_k} + \\alpha}{N_{y_k}+ \\alpha d}$$\n",
    "\n",
    "Where $N_{x_{n'}|y_k}$ is how many times $x_{n'}$ occured given that $y_k$ was observed, $N_{y_k}$ how many times $y_k$ was observed, $\\alpha>0$ is the smoothing parameter ($\\alpha=0$ means no \n",
    "smoothing, many times this is set to 1) and $d$ in the binary case is 2 (because there are two possible values).\n",
    "\n",
    "Knowing the likelihoods and given some prior, one can calculate the posteriors. Using the fact that the sum of posteriors for a given $x$ is 1, the probaility that\n",
    "$y_k$ is observed given $x$ is found. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Implementing Naive Bayes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Building a toy dataset, which tries to discover if the user likes the target movie based on how she likes other three movies (like is YES or NO)\n",
    "X_train = np.array([\n",
    "    [0, 1, 1],\n",
    "    [0, 0, 1],\n",
    "    [0, 0, 0],\n",
    "    [1, 1, 0]])\n",
    "Y_train = ['Y', 'N', 'Y', 'Y']\n",
    "X_test = np.array([[1, 1, 0]])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by label ('Y' and 'N') recording their indices (where they show up) by classes\n",
    "\n",
    "def get_label_indices(labels):\n",
    "    \"\"\"\n",
    "    Group samples based on their labels and return indices\n",
    "    @param labels: list of labels\n",
    "    @return: dict, {class1: [indices], class2: [indices]}\n",
    "    \"\"\"\n",
    "    from collections import defaultdict\n",
    "    label_indices = defaultdict(list)\n",
    "    for index, label in enumerate(labels):\n",
    "        #print('index:',index)\n",
    "        #print('label', label)\n",
    "        label_indices[label].append(index)\n",
    "    return label_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "label_indices:\n defaultdict(<class 'list'>, {'Y': [0, 2, 3], 'N': [1]})\n"
     ]
    }
   ],
   "source": [
    "label_indices = get_label_indices(Y_train)\n",
    "print('label_indices:\\n', label_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prior(label_indices):\n",
    "    \"\"\"\n",
    "    Compute prior based on training examples\n",
    "    @param label_indices: grouped sample indices by class\n",
    "    @return: dictionary, with class label as key, corresponding prior\n",
    "             as the value\n",
    "    \"\"\"\n",
    "    # define prior as an object that can be referred by label\n",
    "    #  get the length of indices inside label_indices items\n",
    "    prior = {label: len(indices) for label, indices in label_indices.items()}\n",
    "    total_count = sum(prior.values())\n",
    "    for label in prior:\n",
    "        prior[label] /= total_count\n",
    "    return prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "index: [0, 2, 3]\nlabel Y\nindex: [1]\nlabel N\nPrior: {'Y': 0.75, 'N': 0.25}\n"
     ]
    }
   ],
   "source": [
    "for label, indices in label_indices.items(): \n",
    "    print('index:',indices)\n",
    "    print('label', label)\n",
    "\n",
    "prior = get_prior(label_indices)\n",
    "print('Prior:', prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_likelihood(features,label_indices,smoothing = 0):\n",
    "    \"\"\"\n",
    "    Compute likelihood based on training samples, using \n",
    "            Laplace approximation\n",
    "    @param features: Matrix of features\n",
    "    @param label_indices: Grouped sample indices by class\n",
    "    @param smoothing: integer, additive smoothing parameter\n",
    "    @return: dictionary, with class as keym corresponding \n",
    "             conditional probability P(feature|class) vector \n",
    "             as value\n",
    "    \"\"\"\n",
    "    likelihood = {}\n",
    "    for label, indices in label_indices.items():\n",
    "\n",
    "        likelihood[label] = features[indices, :].sum(axis=0) + smoothing\n",
    "\n",
    "        total_count = len(indices)\n",
    "\n",
    "        likelihood[label] = likelihood[label]/(total_count + 2 * smoothing)\n",
    "        \n",
    "    return likelihood"
   ]
  },
  {
   "source": [
    "# Setting smoothing = 1\n",
    "smoothing = 1\n",
    "likelihood = get_likelihood(X_train, label_indices, smoothing)\n",
    "print('Likelihood:\\n', likelihood)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 35,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Likelihood:\n {'Y': array([0.4, 0.6, 0.4]), 'N': array([0.33333333, 0.33333333, 0.66666667])}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posterior(X, prior, likelihood):\n",
    "    \"\"\"\n",
    "    Compute posteruor of testing samples, based on prior and likelihood\n",
    "    @param X: testing samples\n",
    "    @param prior: dictionary, with class label as key,\n",
    "                  corresponding prior as the value\n",
    "    @param likelihood: dictionary, with class label as key, \n",
    "                       corresponding conditional value as vector value\n",
    "    @return: dictionary, with class label as key, posterior as value\n",
    "    \"\"\"\n",
    "    posteriors = []\n",
    "    for x in X:\n",
    "        # posterior is proportional to prior * likelihood\n",
    "        posterior = prior.copy()\n",
    "        for label, likelihood_label in likelihood.items():\n",
    "            for index, bool_value in enumerate (x):\n",
    "                posterior[label] *= likelihood_label[index] if bool_value else (1 - likelihood_label[index])\n",
    "        # normalize so that all sums up to 1\n",
    "        sum_posterior = sum(posterior.values())\n",
    "        for label in posterior:\n",
    "            if posterior[label] == float('inf'):\n",
    "                posterior[label] = 1.0\n",
    "            else:\n",
    "                posterior[label] /= sum_posterior\n",
    "        posteriors.append(posterior.copy())\n",
    "    return posteriors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Posterior:\n [{'Y': 0.9210360075805433, 'N': 0.07896399241945673}]\n"
     ]
    }
   ],
   "source": [
    "# From our example\n",
    "posterior = get_posterior(X_test,prior,likelihood)\n",
    "print('Posterior:\\n', posterior)"
   ]
  },
  {
   "source": [
    "Observe that everything we have done above is what Naive Bayes does and we have done it from the scratch.\n",
    "\n",
    "Another possibility is to use *scikit-learn*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Implementing Naive Bayes with scikit-learn\n",
    "\n",
    "- We are going to use BernoulliNB module from scikit-learn\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[scikit-learn] Predicted probabilities:\n [[0.07896399 0.92103601]]\n[scikit-learn] Prediction: ['Y']\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model (alpha is the smoothing factor,\n",
    "#   fit_prior = True means prior learned from the training set)\n",
    "clf = BernoulliNB(alpha = 1.0, fit_prior=True)\n",
    "\n",
    "# Train the classifier\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "# Predict probability results\n",
    "pred_prob = clf.predict_proba(X_test)\n",
    "print('[scikit-learn] Predicted probabilities:\\n', pred_prob)\n",
    "\n",
    "# Or you can also get directly the predicted class by the method\n",
    "pred = clf.predict(X_test)\n",
    "print('[scikit-learn] Prediction:', pred)"
   ]
  },
  {
   "source": [
    "## Evaluating Classification Performance\n",
    "\n",
    "To get more insights in the results, one can look not only to accuracy, but also to:\n",
    "\n",
    "1) **Confusion Matrix** (confusion_matrix from sklearn.metrics)\n",
    "\n",
    "\n",
    "|       |          | Predicted | Predicted|\n",
    "|-------|----------|:---------:|:--------:|\n",
    "|       |          | Negative  | Positive |\n",
    "|**Actual**| Negative |    TN     |    FP    |\n",
    "|**Actual**| Positive |    FN     |    TP    |\n",
    "\n",
    "Where:\n",
    "\n",
    "    - TN: True Negative\n",
    "    \n",
    "    - FP: False Positive\n",
    "\n",
    "    - FN: False Negative\n",
    "\n",
    "    - TP: True Positive\n",
    "\n",
    "1.1) Precision: $\\frac{TP}{TP + FP}$ - Fraction of positive cells measured correctly;\n",
    "\n",
    "1.2) Recall (True positive rate): $\\frac{TP}{TP + FN}$ - Fraction of positive cells correctly identified;\n",
    "\n",
    "1.3) F1 score (harmonic mean): $f_1 = 2 * \\frac{precision * recall}{precision + recall}$\n",
    "\n",
    "\n",
    "These measures can be obtained using scikit-learn by:\n",
    "\n",
    "`sklearn.metrics import precision_score, recall_score, f1_score`\n",
    "\n",
    "or a summarization of all is given by:\n",
    "\n",
    "`from sklearn.metrics import classification_report`\n",
    "\n",
    "\n",
    "\n",
    "2) Area under the curve (AUC) of the receiver operating characteristic (ROC):\n",
    "\n",
    "The distribution of True Positive and True Negative may overlap, if in some regions they do, then AUC is smaller than one. \n",
    "The closer the AUC is to one, the model is perfectly able to distinguish between positive class and negative class. \n",
    "\n",
    "https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5\n",
    "\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Tuning models with cross-validation\n",
    "\n",
    "We usually apply K-fold cross-validation. The idea of K-fold cross-validation is to divide the original dataset into *k* equal-sized subsets, which are retained as a testing set, the remaining k - 1 subsets are used to train model. Then the average performance across all k trials is calculated to generate overall result.\n",
    "\n",
    "Also, cross-validation is used to adjust hyperparameters, boosting learning perfomance and reducing overfitting.\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Chapter 3: Recognizing Faces with Support Vector Machine (SVM)\n",
    "\n",
    "## Finding Separating Boundary with SVM\n",
    "\n",
    "- The whole idea here is to find an optimal hyperplane in order to divide an N-dimensional space of features;\n",
    "\n",
    "- The hyprplane has the characteristic of being in (N-1)-dimensional space, such that if we talk about a 3D space, the hyperplane will be a 2D plane, if we are in a 2D space, the hyperplane will be a line;\n",
    "\n",
    "- Thus the idea is to find the hyperplane that maximizes the distance between the nearest features (points) in the N-dimensional space to the hyperplane. The nearest points delivering this maximization are caled **support vectors**.\n",
    "\n",
    "\n",
    "## Scenario 1: Identifying a separating hyperplane\n",
    "\n",
    "*Definition*: Let $w$ be a n-dimensional vector and $b$ an intercept. A separating hyperplane is such that:\n",
    "\n",
    "- For any data point x from one class, it satisfies $wx + b > 0$\n",
    "\n",
    "- For any data point x from another class, it satisfies $wx + b < 0$\n",
    "\n",
    "\n",
    "## Scenario 2: Determining the optimal hyperplane\n",
    "\n",
    "*Definitions*: \n",
    "\n",
    "- a) The nearest point(s) on the positive side of the hyperplane can constitute parallel hyperplane, this is called **positive hyperplane**. Analogously, in the negative side there is the **negative hyperplane**. The perpendicular distance between the negative and positive hyperplane is called **margin** and a **decision** hyperplane is optimal (or maximum-margin) if maximizes this distance. \n",
    "\n",
    "- b) Mathematically:\n",
    "\n",
    "    - **positive hyperplane**: $w x^{(p)} + b = 1$;\n",
    "\n",
    "    - **negative hyperplane**:: $w x^{(n)} + b = -1$;\n",
    "\n",
    "    - Where $x^{(p)}$ is a point in the positive hyperplane and $x^{(n)}%$ in the negative hyperplane;\n",
    "\n",
    "    - Let $i \\in \\{p,n\\}$. The distance between a point and the decision hyperplane is given by:\n",
    "\n",
    "    $$ \\frac{w x^{(i)} + b}{||w||} = \\frac{1}{||w||} $$\n",
    "\n",
    "    - Margin is then: $\\frac{w x^{(p)} + b}{||w||} +\\frac{w x^{(n)} + b}{||w||} = \\frac{2}{||w||}$;\n",
    "\n",
    "    - Thus we need to minimize $||w||$ to maximize margin.\n",
    "\n",
    "**Optimization problem** - Let $i \\in \\{p,n\\}$:\n",
    "\n",
    "- Minimze $||w||$\n",
    "\n",
    "- Subject to $y^{(i)}(w x^{(i)} + b) \\geq 0$ for the training set $\\{(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)})\\}$;\n",
    "    \n",
    "\n",
    "After optimization,we can use the model as classifier, such that if:\n",
    "\n",
    "- $ \\boldsymbol{w} \\boldsymbol{x}' + b > 0 \\rightarrow 1$;\n",
    "\n",
    "- $ \\boldsymbol{w} \\boldsymbol{x}' + b < 0 \\rightarrow -1$;\n",
    "\n",
    "- $||w x' + b||$ is the confidence of prediction, since it states the distance between the hyperplane and the specific point;\n",
    "\n",
    "\n",
    "## Scenario 3: Handling outliers\n",
    "\n",
    "- Often we cannot separate all the points with a hyperplane, some of them end up in the \"other side\"\n",
    "\n",
    "*Definitions*: \n",
    "\n",
    "- **misclassification error (hinge loss)**: the misclassification measure for a point $i$ is given by:\n",
    "\n",
    "    - i) If misclassified: $\\zeta^{(i)} = 1 - y^{(i)}(w x^{(i)} + b)$;\n",
    "\n",
    "    - ii) Otherwise: $\\zeta^{(i)} = 0$;\n",
    "\n",
    "\n",
    "**Optimization problem** - Let $i \\in \\{p,n\\}$:\n",
    "\n",
    "- Minimze $||w|| + C \\frac{\\sum_{i=1}^m \\zeta^{(i)}}{m}$ \n",
    "\n",
    "- Subject to $y^{(i)}(w x^{(i)} + b) \\geq 0$ for the training set $\\{(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)})\\}$;\n",
    "\n",
    "- Where $C$ is the hyperparameter defining the penalty for misclassification. If high, it will penalize it a lot, making model prone to overfiting, whereas if low, it allows more misclassified points and  possibly lead to low variance but high bias; \n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Implementing SVM\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Loading dataset and performing some basic analysis\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer_data = load_breast_cancer()\n",
    "X = cancer_data.data\n",
    "Y = cancer_data.target\n",
    "print('Input data size:', X.shape)\n",
    "print('Onput data size:', Y.shape)\n",
    "print('Label names:', cancer_data.target_names)\n",
    "n_pos = (Y == 1).sum()\n",
    "n_neg = (Y == 0).sum()\n",
    "print(f' {n_pos} positve samples and {n_neg} negative samples.') # Serves as imbalance check"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input data size: (569, 30)\nOnput data size: (569,)\nLabel names: ['malignant' 'benign']\n 357 positve samples and 212 negative samples.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting training and testing sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SVM classifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel='linear', C=1.0, random_state = 42)\n",
    "# Kernel: will be explained latter on\n",
    "# C: Penalty parameter (default = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "SVC(kernel='linear', random_state=42)"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "# Fit the model to training set\n",
    "\n",
    "clf.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The accuracy is: 95.8%\n"
     ]
    }
   ],
   "source": [
    "accuracy = clf.score(X_test, Y_test)\n",
    "print(f'The accuracy is: {accuracy*100:.1f}%')"
   ]
  },
  {
   "source": [
    "## Scenario 4: Dealing with more than two classes\n",
    "\n",
    "Two typical approaches:\n",
    "\n",
    "1) **one-vc-rest** (or **one-vs-all**)\n",
    "\n",
    "- For a K-class problem run K different binary SVM classifiers, such that if data falls in $k^{th}$ class it assumes one and zero otherwise.\n",
    "\n",
    "- Pick the class delivering higher confidence (larger value of $w_k x' + b_i$):\n",
    "\n",
    "$$y' = argmax_i(w_i x' + b_i)$$\n",
    "\n",
    "2) **one-vs-one**\n",
    "\n",
    "- Pairwise comparisons: Pick two classes (e.g $i$ and $j$) and trains a model on observations from $i$ (as the \"positive case\").\n",
    "Assign class to a new sample. Repeat this process for all paiwise combinations, which results in $\\frac{K(K-1)}{2} different classifiers.\n",
    "\n",
    "- The class that gets the most votes looking to all pairwise classifications is the winner. \n",
    "\n",
    "**Performance**\n",
    "\n",
    "- Generally speaking both have similar accuracy.\n",
    "\n",
    "- one-vs-one tends to be more memory efficient because it trains many models on smaller subset of data.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Get data\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "wine_data = load_wine()\n",
    "X = wine_data.data\n",
    "Y = wine_data.target\n",
    "print('Input data size :', X.shape)\n",
    "print('Output data size :', Y.shape)\n",
    "print('Label names:', wine_data.target_names)\n",
    "n_class0 = (Y == 0).sum()\n",
    "n_class1 = (Y == 1).sum()\n",
    "n_class2 = (Y == 2).sum()\n",
    "print(f'{n_class0} class0 samples,\\n{n_class1} class1 samples,\\n{n_class2} class2 samples.')"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 25,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input data size : (178, 13)\nOutput data size : (178,)\nLabel names: ['class_0' 'class_1' 'class_2']\n59 class0 samples,\n71 class1 samples,\n48 class2 samples.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training and test set\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y,\n",
    "random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "SVC(kernel='linear', random_state=42)"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "# Apply SVM classifier to data, initializing SVC model and iftting against training set\n",
    "\n",
    "clf = SVC(kernel='linear', C=1.0, random_state=42)\n",
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The accuracy is: 97.8%\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on test set\n",
    "\n",
    "accuracy = clf.score(X_test, Y_test)\n",
    "print(f'The accuracy is: {accuracy*100:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        15\n           1       1.00      0.94      0.97        18\n           2       0.92      1.00      0.96        12\n\n    accuracy                           0.98        45\n   macro avg       0.97      0.98      0.98        45\nweighted avg       0.98      0.98      0.98        45\n\n"
     ]
    }
   ],
   "source": [
    "# Check perfornabce for individual classes\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "pred = clf.predict(X_test)\n",
    "print(classification_report(Y_test, pred))"
   ]
  },
  {
   "source": [
    "## Scenario 5: Solving linearly non-separable problems with kernels\n",
    "\n",
    "- There are some classification problems that cannot be solved with linear techniques.\n",
    "\n",
    "- For these cases **SVM with kernels** were invented, they convert the original feature $x^{(i)}$ \n",
    "to a higher dimensional feature space using a function $\\Phi$, such that the transformed dataset $\\Phi(x^{(i)})$\n",
    "is linearly separable. \n",
    "\n",
    "- New observations then become $(\\Phi(x^{(i)}),y^{(i)})$.\n",
    "\n",
    "- Along the process of SVM quadratic optimization, the features are involved in the form of pairwise\n",
    "dot product $x^{(i)} \\cdot x^{(j)}$. It would be more efficient to implement the $\\Phi$ tranformation \n",
    "first on the two low dimensional vectors than on the product itself. A function that satisfies it is\n",
    "called **kernel function**:\n",
    "\n",
    "$$K(x^{(i)},x^{(j)}) = \\Phi(x^{(i)}) \\cdot \\Phi(x^{(j)})$$\n",
    "\n",
    "Several kernels could be used:\n",
    "\n",
    "(i) **Radial Basis Function (RBF)**\n",
    "\n",
    "$$ K(x^{(i)},x^{(j)}) = exp \\left(-\\frac{||x^{(i)} - x^{(j)}||}{2\\sigma^2} \\right) =  exp \\left(-\\gamma ||x^{(i)} - x^{(j)}|| \\right)$$\n",
    "\n",
    "Where $\\gamma = \\frac{1}{2\\sigma^2}$.\n",
    "\n",
    "(ii) **Polynomial** (of degree d)\n",
    "\n",
    "$$ K(x^{(i)},x^{(j)}) = \\left( x^{(i)} \\cdot x^{(j)} + \\gamma \\right)^d $$\n",
    "\n",
    "(iii) **Sigmoid** \n",
    "\n",
    "$$ K(x^{(i)},x^{(j)}) = tanh \\left( x^{(i)} \\cdot x^{(j)} + \\gamma \\right) $$\n",
    "\n",
    "- The parameter $\\gamma$ is the **kernel coefficient** such that a large one (small $\\sigma$)\n",
    "means low variance allowed, representing exact fit on training set which may lead to overfitting. \n",
    "On the other hand, a small $\\gamma$ may lead to loose fit on training set, possibly causing underfitting. \n",
    "\n",
    "- Generally speaking, RBF kernel is preferrable over the others since there is no other parameter to tweak \n",
    "(as $d$ in polynomial) and sigmoid tends to perform similar to RBF (under certain parameters).\n",
    "\n",
    "### Choosing between linear and RBF kernels\n",
    "\n",
    "Three scenarios where linear kernel is favored over RBF:\n",
    "\n",
    "**Scenario 1**: Number of features and instances are high (over than 104 or 105). \n",
    "The dimension of feature space is too high and RBF transformation will not provide \n",
    "performance improvement but will increase computational expense. \n",
    "\n",
    "**Scenario 2**: Number of features in too large when compared to number of training samples.\n",
    "RBF kernel is more prone to overfitting.\n",
    "\n",
    "**Scenario 3**: Number of instances in too large when compared to number of features.\n",
    "RBF kernel for low dimension, generally boost performance, mapping it to higher-dimensional\n",
    "space. Due to training complexity, it becomes inefficient in a training set with more thant\n",
    "106 or 107 samples. \n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}